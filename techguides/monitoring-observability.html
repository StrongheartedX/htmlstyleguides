<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Monitoring &amp; Observability — Tech Guide</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@500;600;700&family=Spline+Sans:wght@300;400;500;600&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        /* ============================================
           ATLAS CONSOLE DESIGN SYSTEM
           Monitoring & Observability Tech Guide
           ============================================
           Clean operational density. Warm canvas.
           Teal accents on off-white. Grid noise.
           Console clarity meets cartographic calm.
           ============================================ */

        :root {
            /* Typography */
            --font-display: "Space Grotesk", sans-serif;
            --font-body: "Spline Sans", sans-serif;
            --font-mono: "IBM Plex Mono", monospace;

            /* Spacing Scale */
            --space-0: 2px;
            --space-1: 4px;
            --space-2: 8px;
            --space-3: 12px;
            --space-4: 16px;
            --space-5: 24px;
            --space-6: 32px;
            --space-7: 48px;
            --space-8: 64px;

            /* Radii */
            --radius-sm: 8px;
            --radius-md: 12px;
            --radius-lg: 18px;

            /* Transitions */
            --ease-snappy: cubic-bezier(0.3, 0.8, 0.2, 1);
            --dur-fast: 140ms;
            --dur-base: 300ms;

            /* Colors */
            --ink: #17212b;
            --ink-soft: #2f3a45;
            --muted: #62707c;
            --surface: #ffffff;
            --surface-alt: #f4f6f0;
            --surface-strong: #ecf0e7;
            --canvas: #f7f5ef;
            --line: rgba(23, 33, 43, 0.12);
            --line-strong: rgba(23, 33, 43, 0.2);

            /* Accent Colors */
            --teal: #1a7f79;
            --teal-light: #e6f3f2;
            --moss: #5f8b5a;
            --amber: #d8a357;
            --clay: #c86b4b;
            --navy: #1c3d5a;

            /* Shadows */
            --shadow-soft: 0 18px 36px rgba(23, 33, 43, 0.14);
            --shadow-cut: 0 2px 0 rgba(23, 33, 43, 0.55);

            /* Alert Colors */
            --alert-info-bg: #e6f3f2;
            --alert-info-border: var(--teal);
            --alert-info-text: #125955;
            --alert-warning-bg: #fdf6e8;
            --alert-warning-border: var(--amber);
            --alert-warning-text: #7a5e1e;
            --alert-danger-bg: #fdf0ec;
            --alert-danger-border: var(--clay);
            --alert-danger-text: #8b3f2a;
            --alert-success-bg: #ecf4eb;
            --alert-success-border: var(--moss);
            --alert-success-text: #3a5c36;

            /* Layout */
            --container-max: 1100px;
            --nav-height: 52px;
        }

        /* ============================================
           RESET & BASE
           ============================================ */

        *, *::before, *::after {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        html {
            font-size: 15px;
            scroll-behavior: smooth;
            scroll-padding-top: 60px;
            -webkit-font-smoothing: antialiased;
        }

        body {
            font-family: var(--font-body);
            font-size: 1rem;
            line-height: 1.6;
            color: var(--ink);
            background:
                linear-gradient(0deg, rgba(255, 255, 255, 0.7), rgba(255, 255, 255, 0.7)),
                radial-gradient(900px 600px at 10% -10%, rgba(26, 127, 121, 0.14), transparent 55%),
                radial-gradient(800px 500px at 90% -10%, rgba(216, 163, 87, 0.12), transparent 60%),
                var(--canvas);
            min-height: 100vh;
            overflow-x: hidden;
        }

        /* Grid noise overlay */
        body::before {
            content: '';
            position: fixed;
            inset: 0;
            background-image:
                linear-gradient(90deg, rgba(23, 33, 43, 0.04) 1px, transparent 1px),
                linear-gradient(0deg, rgba(23, 33, 43, 0.04) 1px, transparent 1px);
            background-size: 120px 120px;
            opacity: 0.5;
            pointer-events: none;
            z-index: 0;
        }

        ::selection {
            background: var(--teal);
            color: var(--surface);
        }

        /* ============================================
           CONTAINER
           ============================================ */

        .container {
            max-width: var(--container-max);
            margin: 0 auto;
            padding: 0 var(--space-6);
            position: relative;
            z-index: 1;
        }

        @media (max-width: 768px) {
            .container {
                padding: 0 var(--space-4);
            }
        }

        /* ============================================
           TYPOGRAPHY
           ============================================ */

        h1, h2, h3, h4, h5, h6 {
            font-family: var(--font-display);
            font-weight: 600;
            line-height: 1.2;
            letter-spacing: -0.01em;
            color: var(--ink);
        }

        h1 { font-size: 2.6rem; font-weight: 700; letter-spacing: -0.025em; }
        h2 { font-size: 1.65rem; margin-bottom: var(--space-5); }
        h3 { font-size: 1.2rem; margin-bottom: var(--space-3); }
        h4 { font-size: 1rem; font-weight: 600; margin-bottom: var(--space-2); }

        p {
            margin-bottom: var(--space-3);
            max-width: 65ch;
            color: var(--ink-soft);
        }

        strong {
            font-weight: 600;
            color: var(--ink);
        }

        .label {
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.12em;
            color: var(--muted);
        }

        /* Inline code */
        code {
            font-family: var(--font-mono);
            font-size: 0.875em;
            background: var(--surface-alt);
            border: 1px solid var(--line);
            padding: 0.15em 0.4em;
            border-radius: 4px;
            color: var(--teal);
        }

        pre code {
            background: none;
            border: none;
            padding: 0;
            color: inherit;
            font-size: 0.85rem;
            border-radius: 0;
        }

        /* ============================================
           BACK LINK
           ============================================ */

        .back-link {
            display: inline-flex;
            align-items: center;
            padding: var(--space-4) var(--space-6);
            font-family: var(--font-mono);
            font-size: 0.8rem;
            font-weight: 500;
            color: var(--muted);
            text-decoration: none;
            transition: color var(--dur-fast) var(--ease-snappy);
            position: relative;
            z-index: 1;
        }

        .back-link:hover {
            color: var(--teal);
        }

        /* ============================================
           SITE HEADER
           ============================================ */

        .site-header {
            padding: var(--space-8) 0 var(--space-7);
            position: relative;
            z-index: 1;
        }

        .site-header .container {
            text-align: center;
        }

        .site-header .label {
            color: var(--teal);
            margin-bottom: var(--space-2);
            display: block;
        }

        .site-header h1 {
            margin-bottom: var(--space-3);
        }

        .site-header .tagline {
            font-size: 1.05rem;
            color: var(--muted);
            max-width: 52ch;
            margin: 0 auto;
            font-weight: 300;
        }

        @media (max-width: 768px) {
            .site-header {
                padding: var(--space-7) 0 var(--space-6);
            }
            .site-header h1 {
                font-size: 1.75rem;
            }
        }

        /* ============================================
           JUMP NAV — Sticky horizontal navigation
           ============================================ */

        .jump-nav {
            position: sticky;
            top: 0;
            z-index: 100;
            background: var(--surface);
            border-top: 1px solid var(--line);
            border-bottom: 1px solid var(--line);
            box-shadow: 0 2px 8px rgba(23, 33, 43, 0.06);
        }

        .jump-nav .container {
            display: flex;
            justify-content: center;
            gap: var(--space-1);
            overflow-x: auto;
            -webkit-overflow-scrolling: touch;
            scrollbar-width: none;
            padding-top: var(--space-2);
            padding-bottom: var(--space-2);
        }

        .jump-nav .container::-webkit-scrollbar {
            display: none;
        }

        .jump-nav a {
            display: inline-flex;
            align-items: center;
            padding: var(--space-2) var(--space-3);
            font-family: var(--font-body);
            font-size: 0.75rem;
            font-weight: 500;
            color: var(--muted);
            text-decoration: none;
            white-space: nowrap;
            border-radius: 999px;
            transition: all var(--dur-fast) var(--ease-snappy);
            letter-spacing: 0.01em;
            flex-shrink: 0;
        }

        .jump-nav a:hover {
            color: var(--teal);
            background: var(--teal-light);
        }

        .jump-nav a:active,
        .jump-nav a.active {
            color: var(--surface);
            background: var(--teal);
        }

        @media (max-width: 768px) {
            .jump-nav .container {
                justify-content: flex-start;
            }

            .jump-nav a {
                padding: var(--space-1) var(--space-2);
                font-size: 0.7rem;
            }
        }

        /* ============================================
           SECTIONS
           ============================================ */

        .section {
            padding: var(--space-7) 0;
            border-bottom: 1px solid var(--line);
            position: relative;
            z-index: 1;
            overflow-x: hidden;
        }

        .section:last-of-type {
            border-bottom: none;
        }

        .section-header {
            display: flex;
            align-items: flex-start;
            gap: var(--space-4);
            margin-bottom: var(--space-6);
        }

        .section-number {
            font-family: var(--font-mono);
            font-size: 2.2rem;
            font-weight: 500;
            color: var(--teal);
            line-height: 1;
            min-width: 48px;
            opacity: 0.7;
        }

        .section-header-text {
            flex: 1;
        }

        .section-title {
            font-family: var(--font-display);
            font-size: 1.65rem;
            font-weight: 700;
            margin-bottom: var(--space-2);
            letter-spacing: -0.02em;
        }

        .section-description {
            font-size: 0.95rem;
            color: var(--muted);
            max-width: 60ch;
            margin: 0;
            font-weight: 300;
        }

        @media (max-width: 768px) {
            .section-header {
                flex-direction: column;
                gap: var(--space-2);
            }
            .section-number {
                font-size: 1.6rem;
            }
            .section-title {
                font-size: 1.35rem;
            }
        }

        /* ============================================
           SUBSECTIONS
           ============================================ */

        .subsection {
            margin-bottom: var(--space-7);
        }

        .subsection:last-child {
            margin-bottom: 0;
        }

        .subsection-title {
            font-family: var(--font-display);
            font-size: 1.15rem;
            font-weight: 600;
            color: var(--ink);
            margin-bottom: var(--space-4);
            padding-bottom: var(--space-2);
            border-bottom: 1px solid var(--line);
        }

        .subsection-subtitle {
            font-family: var(--font-display);
            font-size: 1rem;
            font-weight: 600;
            color: var(--ink-soft);
            margin-bottom: var(--space-3);
            margin-top: var(--space-5);
        }

        .subsection p {
            margin-bottom: var(--space-3);
        }

        /* ============================================
           CODE BLOCKS
           ============================================ */

        pre {
            background: var(--ink);
            color: #d4dae0;
            border-radius: var(--radius-sm);
            padding: var(--space-4) var(--space-5);
            overflow-x: auto;
            margin-bottom: var(--space-4);
            border-left: 3px solid var(--teal);
            font-family: var(--font-mono);
            font-size: 0.85rem;
            line-height: 1.65;
            -webkit-overflow-scrolling: touch;
        }

        pre code {
            color: #d4dae0;
        }

        /* Syntax highlighting colors */
        pre .comment { color: #5c6773; font-style: italic; }
        pre .keyword { color: #5bc0eb; }
        pre .string { color: #98c379; }
        pre .number { color: #d8a357; }
        pre .function { color: #61afef; }
        pre .type { color: #e5c07b; }

        /* ============================================
           TABLES
           ============================================ */

        .table-wrapper {
            overflow-x: auto;
            margin-bottom: var(--space-5);
            border-radius: var(--radius-sm);
            border: 1px solid var(--line);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.875rem;
            line-height: 1.5;
        }

        thead {
            background: var(--surface-alt);
        }

        th {
            font-family: var(--font-display);
            font-weight: 600;
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: var(--ink-soft);
            padding: var(--space-3) var(--space-4);
            text-align: left;
            border-bottom: 2px solid var(--line-strong);
            white-space: nowrap;
        }

        td {
            padding: var(--space-3) var(--space-4);
            border-bottom: 1px solid var(--line);
            color: var(--ink-soft);
            vertical-align: top;
        }

        tbody tr:nth-child(even) {
            background: rgba(244, 246, 240, 0.5);
        }

        tbody tr:hover {
            background: var(--teal-light);
        }

        td code {
            font-size: 0.8rem;
        }

        /* ============================================
           COMMAND GRID
           ============================================ */

        .command-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: var(--space-4);
            margin-bottom: var(--space-5);
        }

        .command-card {
            background: var(--surface);
            border: 1px solid var(--line);
            border-radius: var(--radius-md);
            padding: var(--space-5);
            min-width: 0;
            transition: box-shadow var(--dur-base) var(--ease-snappy),
                        border-color var(--dur-fast) var(--ease-snappy);
        }

        .command-card:hover {
            box-shadow: var(--shadow-soft);
            border-color: var(--line-strong);
        }

        .command-card-title {
            font-family: var(--font-display);
            font-size: 1rem;
            font-weight: 600;
            color: var(--ink);
            margin-bottom: var(--space-1);
            display: flex;
            align-items: center;
            gap: var(--space-2);
        }

        .command-card-desc {
            font-size: 0.875rem;
            color: var(--muted);
            margin-bottom: var(--space-3);
            line-height: 1.5;
        }

        .command-card pre {
            margin-bottom: 0;
            font-size: 0.8rem;
            padding: var(--space-3) var(--space-4);
        }

        @media (max-width: 768px) {
            .command-grid {
                grid-template-columns: 1fr;
            }
        }

        /* ============================================
           ALERTS / CALLOUTS
           ============================================ */

        .alert {
            border-radius: var(--radius-sm);
            padding: var(--space-4) var(--space-5);
            margin-bottom: var(--space-5);
            border-left: 4px solid var(--line-strong);
            background: var(--surface-alt);
        }

        .alert-title {
            font-family: var(--font-display);
            font-size: 0.9rem;
            font-weight: 600;
            margin-bottom: var(--space-2);
        }

        .alert p {
            font-size: 0.9rem;
            margin-bottom: var(--space-2);
            color: inherit;
        }

        .alert p:last-child {
            margin-bottom: 0;
        }

        .alert ul {
            margin: var(--space-2) 0;
            padding-left: var(--space-5);
            font-size: 0.9rem;
        }

        .alert li {
            margin-bottom: var(--space-1);
        }

        .alert-info {
            background: var(--alert-info-bg);
            border-left-color: var(--alert-info-border);
            color: var(--alert-info-text);
        }

        .alert-warning {
            background: var(--alert-warning-bg);
            border-left-color: var(--alert-warning-border);
            color: var(--alert-warning-text);
        }

        .alert-danger {
            background: var(--alert-danger-bg);
            border-left-color: var(--alert-danger-border);
            color: var(--alert-danger-text);
        }

        .alert-success {
            background: var(--alert-success-bg);
            border-left-color: var(--alert-success-border);
            color: var(--alert-success-text);
        }

        /* ============================================
           BADGES
           ============================================ */

        .badge {
            display: inline-flex;
            align-items: center;
            padding: 0.15em 0.55em;
            font-family: var(--font-mono);
            font-size: 0.7rem;
            font-weight: 500;
            border-radius: 999px;
            letter-spacing: 0.03em;
            white-space: nowrap;
        }

        .badge-teal {
            background: var(--teal-light);
            color: var(--teal);
        }

        .badge-amber {
            background: #fdf6e8;
            color: #9a7430;
        }

        .badge-clay {
            background: #fdf0ec;
            color: var(--clay);
        }

        .badge-moss {
            background: #ecf4eb;
            color: var(--moss);
        }

        .badge-navy {
            background: #e8eef4;
            color: var(--navy);
        }

        /* ============================================
           KPI GRID
           ============================================ */

        .kpi-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: var(--space-4);
            margin-bottom: var(--space-5);
        }

        .kpi {
            background: var(--surface);
            border: 1px solid var(--line);
            border-radius: var(--radius-md);
            padding: var(--space-5);
            text-align: center;
            transition: box-shadow var(--dur-base) var(--ease-snappy);
        }

        .kpi:hover {
            box-shadow: var(--shadow-soft);
        }

        .kpi-label {
            font-family: var(--font-display);
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--muted);
            margin-bottom: var(--space-2);
        }

        .kpi-value {
            font-family: var(--font-display);
            font-size: 1.4rem;
            font-weight: 700;
            color: var(--teal);
            margin-bottom: var(--space-2);
        }

        .kpi-desc {
            font-size: 0.82rem;
            color: var(--muted);
            line-height: 1.4;
            margin-bottom: var(--space-2);
        }

        .kpi code {
            font-size: 0.72rem;
            display: block;
            margin-top: var(--space-1);
        }

        @media (max-width: 768px) {
            .kpi-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        @media (max-width: 480px) {
            .kpi-grid {
                grid-template-columns: 1fr;
            }
        }

        /* ============================================
           PILLAR CARDS (for Three Pillars section)
           ============================================ */

        .pillar-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: var(--space-4);
            margin-bottom: var(--space-5);
        }

        .pillar-card {
            background: var(--surface);
            border: 1px solid var(--line);
            border-radius: var(--radius-md);
            padding: var(--space-5);
            border-top: 3px solid var(--teal);
            min-width: 0;
            transition: box-shadow var(--dur-base) var(--ease-snappy);
        }

        .pillar-card:nth-child(2) {
            border-top-color: var(--moss);
        }

        .pillar-card:nth-child(3) {
            border-top-color: var(--amber);
        }

        .pillar-card:hover {
            box-shadow: var(--shadow-soft);
        }

        .pillar-card h4 {
            font-family: var(--font-display);
            font-size: 1.1rem;
            font-weight: 700;
            margin-bottom: var(--space-3);
        }

        .pillar-card p {
            font-size: 0.875rem;
            color: var(--muted);
            margin-bottom: var(--space-3);
        }

        .pillar-card ul {
            list-style: none;
            padding: 0;
        }

        .pillar-card li {
            font-size: 0.85rem;
            color: var(--ink-soft);
            padding: var(--space-1) 0;
            border-bottom: 1px solid var(--line);
        }

        .pillar-card li:last-child {
            border-bottom: none;
        }

        @media (max-width: 768px) {
            .pillar-grid {
                grid-template-columns: 1fr;
            }
        }

        /* ============================================
           METRIC TYPE CARDS
           ============================================ */

        .metric-type {
            background: var(--surface);
            border: 1px solid var(--line);
            border-radius: var(--radius-md);
            padding: var(--space-5);
            margin-bottom: var(--space-5);
            min-width: 0;
        }

        .metric-type-header {
            display: flex;
            align-items: center;
            gap: var(--space-3);
            margin-bottom: var(--space-4);
        }

        .metric-type-icon {
            width: 40px;
            height: 40px;
            border-radius: var(--radius-sm);
            display: flex;
            align-items: center;
            justify-content: center;
            font-family: var(--font-mono);
            font-size: 0.85rem;
            font-weight: 500;
            color: var(--surface);
            flex-shrink: 0;
        }

        .metric-type-icon.counter { background: var(--teal); }
        .metric-type-icon.gauge { background: var(--moss); }
        .metric-type-icon.histogram { background: var(--amber); }
        .metric-type-icon.summary { background: var(--navy); }

        .metric-type h4 {
            font-size: 1.1rem;
            margin-bottom: 0;
        }

        .metric-type p {
            font-size: 0.9rem;
        }

        /* ============================================
           TRACE DIAGRAM
           ============================================ */

        .trace-diagram {
            background: var(--ink);
            color: #d4dae0;
            border-radius: var(--radius-sm);
            padding: var(--space-5);
            margin-bottom: var(--space-5);
            font-family: var(--font-mono);
            font-size: 0.8rem;
            line-height: 1.8;
            overflow-x: auto;
            border-left: 3px solid var(--amber);
        }

        .trace-diagram .span-name { color: #5bc0eb; }
        .trace-diagram .span-time { color: #d8a357; }
        .trace-diagram .span-bar { color: var(--teal); }
        .trace-diagram .span-id { color: #5c6773; }

        /* ============================================
           LISTS
           ============================================ */

        .section ul, .section ol {
            margin-bottom: var(--space-4);
            padding-left: var(--space-5);
        }

        .section li {
            margin-bottom: var(--space-2);
            color: var(--ink-soft);
            font-size: 0.925rem;
        }

        .section li strong {
            color: var(--ink);
        }

        /* ============================================
           SITE FOOTER
           ============================================ */

        .site-footer {
            padding: var(--space-7) 0 var(--space-8);
            text-align: center;
            position: relative;
            z-index: 1;
        }

        .site-footer p {
            font-size: 0.85rem;
            color: var(--muted);
            margin: 0 auto var(--space-3);
        }

        .site-footer a {
            font-family: var(--font-mono);
            font-size: 0.8rem;
            color: var(--teal);
            text-decoration: none;
            transition: color var(--dur-fast);
        }

        .site-footer a:hover {
            color: var(--ink);
        }

        /* ============================================
           CUSTOM SCROLLBARS
           ============================================ */

        /* Firefox */
        * {
            scrollbar-width: thin;
            scrollbar-color: var(--teal) var(--canvas);
        }

        /* WebKit (Chrome, Safari, Edge) */
        ::-webkit-scrollbar {
            width: 10px;
            height: 10px;
        }

        ::-webkit-scrollbar-track {
            background: var(--canvas);
        }

        ::-webkit-scrollbar-thumb {
            background: linear-gradient(180deg, var(--teal), var(--moss));
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--navy);
        }

        ::-webkit-scrollbar-corner {
            background: var(--canvas);
        }

        /* Code block scrollbars */
        pre::-webkit-scrollbar {
            height: 6px;
        }

        pre::-webkit-scrollbar-track {
            background: var(--ink);
        }

        pre::-webkit-scrollbar-thumb {
            background: var(--ink-soft);
            border-radius: 3px;
        }

        pre::-webkit-scrollbar-thumb:hover {
            background: var(--muted);
        }

        /* ============================================
           RESPONSIVE BREAKPOINTS
           ============================================ */

        @media (max-width: 768px) {
            html {
                font-size: 14px;
            }

            .section {
                padding: var(--space-6) 0;
            }

            .section-header {
                margin-bottom: var(--space-5);
            }

            pre {
                padding: var(--space-3) var(--space-4);
                font-size: 0.8rem;
                border-radius: var(--radius-sm);
            }

            table {
                font-size: 0.8rem;
            }

            th, td {
                padding: var(--space-2) var(--space-3);
            }

            .kpi-value {
                font-size: 1.2rem;
            }

            .pillar-grid {
                grid-template-columns: 1fr;
            }

            .command-grid {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 480px) {
            h1 {
                font-size: 1.8rem;
            }

            .section-number {
                font-size: 1.4rem;
            }

            .section-title {
                font-size: 1.2rem;
            }

            .kpi-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>

    <a href="index.html" class="back-link">&larr; Tech Guides</a>

    <!-- ============================================
         HEADER
         ============================================ -->

    <header class="site-header">
        <div class="container">
            <span class="label">Atlas Console / Observability Reference</span>
            <h1>Monitoring &amp; Observability</h1>
            <p class="tagline">Metrics, logs, and traces for understanding distributed systems. From Prometheus queries to incident response runbooks.</p>
        </div>
    </header>

    <!-- ============================================
         JUMP NAV
         ============================================ -->

    <nav class="jump-nav">
        <div class="container">
            <a href="#cheatsheet">Quick Reference</a>
            <a href="#pillars">Three Pillars</a>
            <a href="#metrics">Metrics</a>
            <a href="#logging">Logging</a>
            <a href="#tracing">Tracing</a>
            <a href="#alerting">Alerting</a>
            <a href="#dashboards">Dashboards</a>
            <a href="#opentelemetry">OpenTelemetry</a>
            <a href="#infrastructure">Infrastructure</a>
            <a href="#slos">SLIs/SLOs</a>
            <a href="#incidents">Incidents</a>
            <a href="#ecosystem">Ecosystem</a>
        </div>
    </nav>

    <!-- ============================================
         SECTION 01 — QUICK REFERENCE
         ============================================ -->

    <section id="cheatsheet" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">01</span>
                <div class="section-header-text">
                    <h2 class="section-title">Quick Reference</h2>
                    <p class="section-description">Key signals, essential tools, and monitoring methodologies at a glance.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Key Signals at a Glance -->
                <div class="subsection">
                    <h3 class="subsection-title">Key Signals at a Glance &mdash; The Four Golden Signals</h3>

                    <div class="kpi-grid">
                        <div class="kpi">
                            <div class="kpi-label">Latency</div>
                            <div class="kpi-value">P95 / P99</div>
                            <div class="kpi-desc">Response time for successful requests. Distinguish between successful and failed request latency.</div>
                            <code>histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))</code>
                        </div>
                        <div class="kpi">
                            <div class="kpi-label">Traffic</div>
                            <div class="kpi-value">req/s</div>
                            <div class="kpi-desc">Demand on your system measured in requests per second, transactions, or sessions.</div>
                            <code>sum(rate(http_requests_total[5m]))</code>
                        </div>
                        <div class="kpi">
                            <div class="kpi-label">Errors</div>
                            <div class="kpi-value">% rate</div>
                            <div class="kpi-desc">Rate of requests that fail, either explicitly (5xx) or implicitly (wrong content, slow responses).</div>
                            <code>sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100</code>
                        </div>
                        <div class="kpi">
                            <div class="kpi-label">Saturation</div>
                            <div class="kpi-value">% util</div>
                            <div class="kpi-desc">How full your service is. CPU, memory, disk I/O, network bandwidth utilization.</div>
                            <code>node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100</code>
                        </div>
                    </div>
                </div>

                <!-- Essential Tools Quick Reference -->
                <div class="subsection">
                    <h3 class="subsection-title">Essential Tools Quick Reference</h3>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Metrics</span>
                                Prometheus
                            </div>
                            <p class="command-card-desc">Open-source metrics collection and alerting toolkit. Pull-based model with powerful PromQL query language.</p>
                            <pre><code># Check targets status
curl localhost:9090/api/v1/targets

# Instant query
curl 'localhost:9090/api/v1/query?query=up'

# Range query (last 1h, 15s step)
curl 'localhost:9090/api/v1/query_range?query=rate(http_requests_total[5m])&start=2024-01-01T00:00:00Z&end=2024-01-01T01:00:00Z&step=15s'</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Visualization</span>
                                Grafana
                            </div>
                            <p class="command-card-desc">Open-source analytics and visualization platform. Dashboards for Prometheus, Loki, Tempo, and 100+ data sources.</p>
                            <pre><code># Start Grafana (Docker)
docker run -d -p 3000:3000 \
  --name grafana \
  grafana/grafana-oss

# Provision dashboards via API
curl -X POST -H "Content-Type: application/json" \
  -d @dashboard.json \
  http://admin:admin@localhost:3000/api/dashboards/db</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Logs</span>
                                Loki
                            </div>
                            <p class="command-card-desc">Log aggregation system inspired by Prometheus. Indexes labels, not content. Cost-efficient at scale.</p>
                            <pre><code># LogQL — query logs by labels
{job="api-server"} |= "error"

# Rate of log lines with errors
rate({job="api-server"} |= "error" [5m])

# Extract and aggregate fields
{job="api"} | json | status >= 500
  | line_format "{{.method}} {{.path}}"</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Tracing</span>
                                Tempo
                            </div>
                            <p class="command-card-desc">Distributed tracing backend. Accepts Jaeger, Zipkin, and OpenTelemetry formats. Minimal indexing, object storage.</p>
                            <pre><code># Search traces by service name
curl 'localhost:3200/api/search?tags=service.name%3Dapi-server'

# Get trace by ID
curl 'localhost:3200/api/traces/2f70c82d4a2c7e8a'

# TraceQL query
{ span.http.status_code >= 500 }
  | select(span.http.url)</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Standard</span>
                                OpenTelemetry
                            </div>
                            <p class="command-card-desc">Vendor-neutral instrumentation standard. APIs, SDKs, and the Collector for metrics, logs, and traces.</p>
                            <pre><code># Install OTel Python SDK
pip install opentelemetry-api \
  opentelemetry-sdk \
  opentelemetry-exporter-otlp

# Run the OTel Collector
docker run -p 4317:4317 -p 4318:4318 \
  -v ./otel-config.yaml:/etc/otelcol/config.yaml \
  otel/opentelemetry-collector-contrib</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Alerting</span>
                                Alertmanager
                            </div>
                            <p class="command-card-desc">Handles deduplication, grouping, routing, silencing, and inhibition of alerts from Prometheus and other sources.</p>
                            <pre><code># Check active alerts
curl localhost:9093/api/v2/alerts

# Silence an alert (JSON body)
curl -X POST localhost:9093/api/v2/silences \
  -d '{"matchers":[{"name":"alertname",
  "value":"HighLatency","isRegex":false}],
  "startsAt":"2024-01-01T00:00:00Z",
  "endsAt":"2024-01-01T06:00:00Z",
  "createdBy":"oncall","comment":"deploy"}'</code></pre>
                        </div>
                    </div>
                </div>

                <!-- Methodology Comparison -->
                <div class="subsection">
                    <h3 class="subsection-title">Monitoring Methodology Comparison</h3>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Focus</th>
                                    <th>Best For</th>
                                    <th>Key Metrics</th>
                                    <th>Origin</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Golden Signals</strong></td>
                                    <td>System health</td>
                                    <td>SRE monitoring</td>
                                    <td><code>Latency</code>, <code>Traffic</code>, <code>Errors</code>, <code>Saturation</code></td>
                                    <td>Google SRE Book</td>
                                </tr>
                                <tr>
                                    <td><strong>RED Method</strong></td>
                                    <td>User experience</td>
                                    <td>Services / APIs</td>
                                    <td><code>Rate</code>, <code>Errors</code>, <code>Duration</code></td>
                                    <td>Tom Wilkie (Grafana)</td>
                                </tr>
                                <tr>
                                    <td><strong>USE Method</strong></td>
                                    <td>Resource health</td>
                                    <td>Infrastructure / Hardware</td>
                                    <td><code>Utilization</code>, <code>Saturation</code>, <code>Errors</code></td>
                                    <td>Brendan Gregg</td>
                                </tr>
                                <tr>
                                    <td><strong>LETS</strong></td>
                                    <td>Holistic view</td>
                                    <td>Combined approach</td>
                                    <td><code>Latency</code>, <code>Errors</code>, <code>Traffic</code>, <code>Saturation</code></td>
                                    <td>Community evolution</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="alert alert-info">
                        <div class="alert-title">Which method should I use?</div>
                        <p>Use <strong>RED</strong> for microservices and user-facing APIs. Use <strong>USE</strong> for infrastructure (CPU, disks, network). Use <strong>Golden Signals</strong> as a universal starting point that covers both. Most teams combine approaches: RED for services, USE for the machines running them.</p>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 02 — THE THREE PILLARS
         ============================================ -->

    <section id="pillars" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">02</span>
                <div class="section-header-text">
                    <h2 class="section-title">The Three Pillars</h2>
                    <p class="section-description">Metrics, logs, and traces form the foundation of observability. Each pillar provides a different lens into system behavior.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Pillar Overview Cards -->
                <div class="pillar-grid">
                    <div class="pillar-card">
                        <h4>Metrics <span class="badge badge-teal">Aggregated</span></h4>
                        <p>Numeric measurements collected at regular intervals and aggregated over time. Compact, fast to query, ideal for dashboards and alerting.</p>
                        <ul>
                            <li><strong>Data model:</strong> Time-series (timestamp + value + labels)</li>
                            <li><strong>Storage cost:</strong> Low (fixed bytes per sample)</li>
                            <li><strong>Query speed:</strong> Very fast (pre-aggregated)</li>
                            <li><strong>Best for:</strong> Trends, alerting, dashboards</li>
                            <li><strong>Tools:</strong> Prometheus, Datadog, CloudWatch, InfluxDB</li>
                        </ul>
                    </div>
                    <div class="pillar-card">
                        <h4>Logs <span class="badge badge-moss">Discrete</span></h4>
                        <p>Timestamped text records of discrete events. Rich context per event, variable size. Essential for debugging and audit trails.</p>
                        <ul>
                            <li><strong>Data model:</strong> Timestamped text (structured or unstructured)</li>
                            <li><strong>Storage cost:</strong> High (unbounded per event)</li>
                            <li><strong>Query speed:</strong> Moderate (full-text search)</li>
                            <li><strong>Best for:</strong> Debugging, audit trails, event investigation</li>
                            <li><strong>Tools:</strong> Loki, ELK Stack, CloudWatch Logs, Splunk</li>
                        </ul>
                    </div>
                    <div class="pillar-card">
                        <h4>Traces <span class="badge badge-amber">Distributed</span></h4>
                        <p>End-to-end request paths through distributed systems. Shows the causal chain of operations across service boundaries.</p>
                        <ul>
                            <li><strong>Data model:</strong> Trace &rarr; Spans (parent-child tree)</li>
                            <li><strong>Storage cost:</strong> Medium (sampled, per-request)</li>
                            <li><strong>Query speed:</strong> Fast by ID, slower by search</li>
                            <li><strong>Best for:</strong> Latency analysis, bottleneck detection</li>
                            <li><strong>Tools:</strong> Jaeger, Tempo, Zipkin, AWS X-Ray</li>
                        </ul>
                    </div>
                </div>

                <!-- Metrics Deep Dive -->
                <div class="subsection">
                    <h3 class="subsection-title">Metrics &mdash; Numeric Signals Over Time</h3>

                    <p>Metrics are the workhorses of monitoring. Each data point is a numeric value with a timestamp and a set of key-value labels. Because they are fixed-size and pre-aggregated, they are extremely storage-efficient and fast to query.</p>

                    <p><strong>Time-series structure:</strong> A metric is identified by its name and label set. Each unique combination of name + labels is a separate time series.</p>

<pre><code># Prometheus exposition format
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET", handler="/api/users", status="200"} 14523
http_requests_total{method="POST", handler="/api/users", status="201"} 892
http_requests_total{method="GET", handler="/api/users", status="500"} 37

# Each unique label combination = one time series
# High cardinality labels (user_id, request_id) are dangerous
# Stick to bounded label values (method, status, handler)</code></pre>

                    <p><strong>Example metrics for a web service:</strong></p>
                    <ul>
                        <li><code>http_requests_total</code> &mdash; Total request count (counter)</li>
                        <li><code>http_request_duration_seconds</code> &mdash; Request latency (histogram)</li>
                        <li><code>http_requests_in_flight</code> &mdash; Current active requests (gauge)</li>
                        <li><code>process_cpu_seconds_total</code> &mdash; CPU time consumed (counter)</li>
                        <li><code>process_resident_memory_bytes</code> &mdash; Memory usage (gauge)</li>
                    </ul>
                </div>

                <!-- Logs Deep Dive -->
                <div class="subsection">
                    <h3 class="subsection-title">Logs &mdash; Discrete Event Records</h3>

                    <p>Logs capture what happened at a specific moment in time. They range from simple unstructured text lines to richly structured JSON objects. The key distinction is between <strong>structured</strong> (machine-parseable) and <strong>unstructured</strong> (human-readable) formats.</p>

                    <h4 class="subsection-subtitle">Structured Logging (JSON)</h4>

<pre><code>{
  "timestamp": "2024-03-15T14:23:45.123Z",
  "level": "ERROR",
  "service": "payment-api",
  "trace_id": "abc123def456",
  "span_id": "789ghi012",
  "message": "Payment processing failed",
  "error": {
    "type": "PaymentGatewayTimeout",
    "message": "Upstream timeout after 30s",
    "stack": "PaymentGatewayTimeout: at processPayment (payment.js:142)"
  },
  "context": {
    "user_id": "usr_8x7k2m",
    "order_id": "ord_3f9a1c",
    "amount_cents": 4999,
    "currency": "USD",
    "gateway": "stripe",
    "retry_count": 2
  }
}</code></pre>

                    <h4 class="subsection-subtitle">Unstructured Logging (Plain Text)</h4>

<pre><code># Traditional syslog / application logs
2024-03-15 14:23:45 ERROR [payment-api] Payment processing failed for order ord_3f9a1c: upstream timeout after 30s (retry 2/3)
2024-03-15 14:23:45 WARN  [payment-api] Circuit breaker for stripe gateway at 80% threshold
2024-03-15 14:23:46 INFO  [payment-api] Falling back to backup payment gateway for user usr_8x7k2m</code></pre>

                    <div class="alert alert-warning">
                        <div class="alert-title">Always prefer structured logging</div>
                        <p>Structured logs (JSON) are dramatically easier to query, filter, and aggregate. They enable LogQL/Splunk field extraction without regex parsing. The cost of structured logging is slightly larger payloads, but the operational benefits far outweigh the storage cost.</p>
                    </div>
                </div>

                <!-- Traces Deep Dive -->
                <div class="subsection">
                    <h3 class="subsection-title">Traces &mdash; Request Paths Through Distributed Systems</h3>

                    <p>A <strong>trace</strong> represents the complete journey of a single request through a distributed system. It consists of multiple <strong>spans</strong>, each representing a unit of work. Spans have parent-child relationships that form a tree structure, showing the causal chain of operations.</p>

                    <div class="trace-diagram">
<span class="span-id">Trace ID: 7a3f2b1c4d5e6f89</span>

<span class="span-name">api-gateway</span>  <span class="span-bar">|====================|</span>                         <span class="span-time">240ms</span>
<span class="span-name">  auth-svc</span>    <span class="span-bar">  |====|</span>                                         <span class="span-time"> 35ms</span>
<span class="span-name">  user-svc</span>    <span class="span-bar">        |===========|</span>                            <span class="span-time"> 85ms</span>
<span class="span-name">    user-db</span>   <span class="span-bar">          |=======|</span>                              <span class="span-time"> 62ms</span>
<span class="span-name">    cache</span>     <span class="span-bar">                    |=|</span>                         <span class="span-time">  8ms</span>
<span class="span-name">  order-svc</span>   <span class="span-bar">                      |==========|</span>             <span class="span-time"> 90ms</span>
<span class="span-name">    order-db</span>  <span class="span-bar">                        |=====|</span>                <span class="span-time"> 45ms</span>
<span class="span-name">    payment</span>   <span class="span-bar">                              |====|</span>          <span class="span-time"> 30ms</span>
                    </div>

                    <p><strong>Key trace concepts:</strong></p>
                    <ul>
                        <li><strong>Trace:</strong> A collection of spans sharing a single trace ID, representing one end-to-end request</li>
                        <li><strong>Span:</strong> A single operation within a trace (e.g., an HTTP call, a DB query, a function invocation)</li>
                        <li><strong>Context propagation:</strong> Trace/span IDs are passed via HTTP headers (<code>traceparent</code>, <code>b3</code>) across service boundaries</li>
                        <li><strong>Sampling:</strong> Most systems sample traces (1-10%) to manage storage costs while retaining statistical significance</li>
                        <li><strong>Span attributes:</strong> Key-value metadata on each span (<code>http.method</code>, <code>db.statement</code>, <code>error</code>)</li>
                    </ul>
                </div>

                <!-- When to Use Which Pillar -->
                <div class="subsection">
                    <h3 class="subsection-title">When to Use Which Pillar</h3>

                    <div class="alert alert-info">
                        <div class="alert-title">Decision guide for choosing the right signal</div>
                        <ul>
                            <li><strong>"Is the system healthy right now?"</strong> &rarr; Metrics (dashboards, alerts)</li>
                            <li><strong>"What happened at 3:42 AM?"</strong> &rarr; Logs (event investigation)</li>
                            <li><strong>"Why is this specific request slow?"</strong> &rarr; Traces (latency breakdown)</li>
                            <li><strong>"What is the trend over the past week?"</strong> &rarr; Metrics (time-series queries)</li>
                            <li><strong>"Which service is the bottleneck?"</strong> &rarr; Traces (span analysis)</li>
                            <li><strong>"What changed in the config?"</strong> &rarr; Logs (audit trail)</li>
                            <li><strong>"How many users hit this error?"</strong> &rarr; Metrics (counters) + Logs (context)</li>
                        </ul>
                    </div>
                </div>

                <!-- Comparison Table -->
                <div class="subsection">
                    <h3 class="subsection-title">Pillar Comparison</h3>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Attribute</th>
                                    <th>Metrics</th>
                                    <th>Logs</th>
                                    <th>Traces</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Data Model</strong></td>
                                    <td>Numeric time-series (name + labels + value)</td>
                                    <td>Timestamped text records (structured/unstructured)</td>
                                    <td>Span trees with parent-child relationships</td>
                                </tr>
                                <tr>
                                    <td><strong>Storage Cost</strong></td>
                                    <td><span class="badge badge-moss">Low</span> ~1-2 bytes/sample</td>
                                    <td><span class="badge badge-clay">High</span> Unbounded per event</td>
                                    <td><span class="badge badge-amber">Medium</span> ~1-5 KB/span (sampled)</td>
                                </tr>
                                <tr>
                                    <td><strong>Query Speed</strong></td>
                                    <td>Very fast (pre-aggregated TSDB)</td>
                                    <td>Moderate (full-text index/search)</td>
                                    <td>Fast by trace ID, slower by attribute search</td>
                                </tr>
                                <tr>
                                    <td><strong>Cardinality</strong></td>
                                    <td>Bounded (labels must be low-cardinality)</td>
                                    <td>Unbounded (any key-value pair)</td>
                                    <td>Per-request (sampled to control volume)</td>
                                </tr>
                                <tr>
                                    <td><strong>Best Use Case</strong></td>
                                    <td>Alerting, trends, capacity planning</td>
                                    <td>Debugging, audit, event forensics</td>
                                    <td>Latency analysis, dependency mapping</td>
                                </tr>
                                <tr>
                                    <td><strong>Retention</strong></td>
                                    <td>Weeks to years (downsampled)</td>
                                    <td>Days to months</td>
                                    <td>Days to weeks</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 03 — METRICS & TIME SERIES
         ============================================ -->

    <section id="metrics" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">03</span>
                <div class="section-header-text">
                    <h2 class="section-title">Metrics &amp; Time Series</h2>
                    <p class="section-description">Deep dive into Prometheus metric types, PromQL queries, recording rules, and architecture patterns.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Metric Types -->
                <div class="subsection">
                    <h3 class="subsection-title">Metric Types</h3>

                    <p>Prometheus defines four core metric types. Choosing the right type is critical for accurate instrumentation and meaningful queries.</p>

                    <!-- Counter -->
                    <div class="metric-type">
                        <div class="metric-type-header">
                            <div class="metric-type-icon counter">C</div>
                            <div>
                                <h4>Counter <span class="badge badge-teal">Monotonic</span></h4>
                                <p style="margin-bottom: 0;">A value that only goes up (and resets to zero on process restart). Use <code>rate()</code> to get the per-second rate of increase.</p>
                            </div>
                        </div>

                        <p><strong>When to use:</strong> Total requests served, total errors, total bytes transferred. Anything that accumulates over time.</p>

                        <h4 class="subsection-subtitle">Python Instrumentation</h4>

<pre><code>from prometheus_client import Counter

# Define a counter with labels
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'handler', 'status']
)

# Increment in your request handler
@app.route('/api/users', methods=['GET'])
def get_users():
    try:
        users = db.query_users()
        http_requests_total.labels(
            method='GET',
            handler='/api/users',
            status='200'
        ).inc()
        return jsonify(users), 200
    except Exception as e:
        http_requests_total.labels(
            method='GET',
            handler='/api/users',
            status='500'
        ).inc()
        raise</code></pre>

                        <h4 class="subsection-subtitle">PromQL Queries for Counters</h4>

<pre><code># Per-second request rate over last 5 minutes
rate(http_requests_total[5m])

# Per-second rate, per handler
sum by (handler) (rate(http_requests_total[5m]))

# Total requests in the last hour
increase(http_requests_total[1h])

# Error rate as a percentage
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))
* 100</code></pre>

                        <div class="alert alert-danger">
                            <div class="alert-title">Never use a raw counter value in alerts</div>
                            <p>Counter values are monotonically increasing and reset on restart. Always wrap counters with <code>rate()</code>, <code>irate()</code>, or <code>increase()</code>. Plotting a raw counter produces a meaningless ever-climbing line.</p>
                        </div>
                    </div>

                    <!-- Gauge -->
                    <div class="metric-type">
                        <div class="metric-type-header">
                            <div class="metric-type-icon gauge">G</div>
                            <div>
                                <h4>Gauge <span class="badge badge-moss">Point-in-Time</span></h4>
                                <p style="margin-bottom: 0;">A value that can go up and down. Represents a current snapshot: temperature, queue depth, active connections.</p>
                            </div>
                        </div>

                        <p><strong>When to use:</strong> Current memory usage, active connections, queue length, temperature, number of goroutines. Anything with a "current value."</p>

                        <h4 class="subsection-subtitle">Python Instrumentation</h4>

<pre><code>from prometheus_client import Gauge

# Simple gauge
requests_in_flight = Gauge(
    'http_requests_in_flight',
    'Number of HTTP requests currently being processed',
    ['handler']
)

# Use as context manager for automatic inc/dec
@app.route('/api/process')
def process():
    with requests_in_flight.labels(handler='/api/process').track_inprogress():
        result = expensive_computation()
        return jsonify(result)

# Set to an absolute value (e.g., from a sensor)
queue_depth = Gauge('job_queue_depth', 'Current job queue depth')
queue_depth.set(len(pending_jobs))

# Use set_function for callback-based collection
cpu_usage = Gauge('process_cpu_percent', 'CPU usage percentage')
cpu_usage.set_function(lambda: psutil.cpu_percent())</code></pre>

                        <h4 class="subsection-subtitle">PromQL Queries for Gauges</h4>

<pre><code># Current value
http_requests_in_flight

# Average over the last 5 minutes
avg_over_time(http_requests_in_flight[5m])

# Max value in the last hour
max_over_time(http_requests_in_flight[1h])

# Rate of change (useful for detecting sudden spikes)
deriv(http_requests_in_flight[5m])

# Predict value 4 hours from now (linear regression)
predict_linear(node_filesystem_avail_bytes[6h], 4*3600)</code></pre>
                    </div>

                    <!-- Histogram -->
                    <div class="metric-type">
                        <div class="metric-type-header">
                            <div class="metric-type-icon histogram">H</div>
                            <div>
                                <h4>Histogram <span class="badge badge-amber">Distribution</span></h4>
                                <p style="margin-bottom: 0;">Samples observations and counts them in configurable buckets. Enables percentile calculations server-side.</p>
                            </div>
                        </div>

                        <p><strong>When to use:</strong> Request latency, response sizes, batch job durations. Anything where you need percentiles (P50, P95, P99) or distribution analysis.</p>

                        <h4 class="subsection-subtitle">Exposition Format (What Prometheus Scrapes)</h4>

<pre><code># HELP http_request_duration_seconds HTTP request latency
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{handler="/api",le="0.005"}   3240
http_request_duration_seconds_bucket{handler="/api",le="0.01"}    4521
http_request_duration_seconds_bucket{handler="/api",le="0.025"}   6892
http_request_duration_seconds_bucket{handler="/api",le="0.05"}    8341
http_request_duration_seconds_bucket{handler="/api",le="0.1"}     9102
http_request_duration_seconds_bucket{handler="/api",le="0.25"}    9498
http_request_duration_seconds_bucket{handler="/api",le="0.5"}     9612
http_request_duration_seconds_bucket{handler="/api",le="1"}       9645
http_request_duration_seconds_bucket{handler="/api",le="+Inf"}    9650
http_request_duration_seconds_sum{handler="/api"}                  298.45
http_request_duration_seconds_count{handler="/api"}                9650</code></pre>

                        <h4 class="subsection-subtitle">Python Instrumentation</h4>

<pre><code>from prometheus_client import Histogram

# Default buckets: .005, .01, .025, .05, .075, .1, .25, .5, .75, 1, 2.5, 5, 7.5, 10
http_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency in seconds',
    ['method', 'handler'],
    buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

# Observe with decorator
@http_duration.labels(method='GET', handler='/api/users').time()
def get_users():
    return db.query_users()

# Or manually observe
start = time.time()
result = process_request()
http_duration.labels(method='POST', handler='/api/orders').observe(time.time() - start)</code></pre>

                        <h4 class="subsection-subtitle">PromQL for Percentiles</h4>

<pre><code># P95 latency across all handlers
histogram_quantile(0.95,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)

# P99 latency per handler
histogram_quantile(0.99,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le, handler)
)

# P50 (median) latency
histogram_quantile(0.50,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)

# Average latency (from sum and count)
rate(http_request_duration_seconds_sum[5m])
/
rate(http_request_duration_seconds_count[5m])

# Apdex score (satisfied &lt; 0.25s, tolerating &lt; 1s)
(
  sum(rate(http_request_duration_seconds_bucket{le="0.25"}[5m]))
  +
  sum(rate(http_request_duration_seconds_bucket{le="1"}[5m]))
) / 2
/
sum(rate(http_request_duration_seconds_count[5m]))</code></pre>
                    </div>

                    <!-- Summary -->
                    <div class="metric-type">
                        <div class="metric-type-header">
                            <div class="metric-type-icon summary">S</div>
                            <div>
                                <h4>Summary <span class="badge badge-navy">Client-side</span></h4>
                                <p style="margin-bottom: 0;">Calculates quantiles on the client side. Pre-configured percentiles cannot be aggregated across instances.</p>
                            </div>
                        </div>

                        <p><strong>When to use:</strong> Rarely. Prefer histograms in almost all cases. Summaries are useful only when you need exact quantiles for a single instance and cannot accept the bucket approximation error.</p>

                        <h4 class="subsection-subtitle">Histogram vs Summary Comparison</h4>

                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Feature</th>
                                        <th>Histogram</th>
                                        <th>Summary</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Quantile calculation</strong></td>
                                        <td>Server-side (PromQL)</td>
                                        <td>Client-side (pre-computed)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Aggregation across instances</strong></td>
                                        <td><span class="badge badge-moss">Yes</span> (aggregate buckets, then quantile)</td>
                                        <td><span class="badge badge-clay">No</span> (cannot average percentiles)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Accuracy</strong></td>
                                        <td>Approximated by bucket boundaries</td>
                                        <td>Exact for configured quantiles</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Configuration</strong></td>
                                        <td>Bucket boundaries (can change later)</td>
                                        <td>Quantile targets (fixed at instrumentation time)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Cost on client</strong></td>
                                        <td>Low (simple counter increments)</td>
                                        <td>Higher (streaming quantile estimation)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Recommendation</strong></td>
                                        <td><span class="badge badge-teal">Preferred</span></td>
                                        <td>Use only if aggregation is unnecessary</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                </div>

                <!-- PromQL Essentials -->
                <div class="subsection">
                    <h3 class="subsection-title">PromQL Essentials</h3>

                    <p>PromQL (Prometheus Query Language) is a functional query language for selecting and aggregating time-series data. Understanding <code>rate()</code> vs <code>irate()</code> vs <code>increase()</code> is the single most important PromQL skill.</p>

                    <h4 class="subsection-subtitle">rate() vs irate() vs increase()</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Function</th>
                                    <th>Behavior</th>
                                    <th>Best For</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>rate(v[d])</code></td>
                                    <td>Average per-second rate over range <code>d</code></td>
                                    <td>Alerting, recording rules, dashboards</td>
                                    <td><code>rate(http_requests_total[5m])</code></td>
                                </tr>
                                <tr>
                                    <td><code>irate(v[d])</code></td>
                                    <td>Instantaneous per-second rate (last two samples)</td>
                                    <td>Volatile, fast-moving counters on dashboards</td>
                                    <td><code>irate(http_requests_total[5m])</code></td>
                                </tr>
                                <tr>
                                    <td><code>increase(v[d])</code></td>
                                    <td>Total increase over range <code>d</code> (= rate * seconds)</td>
                                    <td>"How many X happened in the last hour?"</td>
                                    <td><code>increase(http_requests_total[1h])</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="alert alert-info">
                        <div class="alert-title">Best practice: range window = 4x scrape interval</div>
                        <p>If your scrape interval is 15s, use at least <code>[1m]</code> ranges for <code>rate()</code>. The rule of thumb is <strong>4x the scrape interval</strong> to ensure you always have at least two data points in the range, even after a failed scrape. For 15s intervals: <code>rate(metric[1m])</code>. For 30s intervals: <code>rate(metric[2m])</code>.</p>
                    </div>

                    <h4 class="subsection-subtitle">Common Query Patterns</h4>

<pre><code># --- Error Rate Percentage ---
# 5xx error rate as a percentage of all requests
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))
* 100

# --- P95 Latency ---
histogram_quantile(0.95,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)

# --- Request Rate by Status Code ---
sum by (status) (rate(http_requests_total[5m]))

# --- Top 5 Endpoints by Request Rate ---
topk(5, sum by (handler) (rate(http_requests_total[5m])))

# --- Memory Usage Percentage ---
(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

# --- CPU Usage Per Core ---
1 - avg by (cpu) (rate(node_cpu_seconds_total{mode="idle"}[5m]))

# --- Disk Space Prediction (hours until full) ---
predict_linear(node_filesystem_avail_bytes{mountpoint="/"}[6h], 24*3600) &lt; 0

# --- Absent Alert (target is down) ---
absent(up{job="api-server"}) == 1

# --- Multi-dimensional aggregation ---
# Average latency by service AND method
avg by (service, method) (
  rate(http_request_duration_seconds_sum[5m])
  /
  rate(http_request_duration_seconds_count[5m])
)</code></pre>

                    <h4 class="subsection-subtitle">Recording Rules</h4>

                    <p>Recording rules pre-compute expensive PromQL expressions and save the results as new time series. This improves dashboard load times and enables composable alerting.</p>

<pre><code># prometheus-rules.yaml
groups:
  - name: http_recording_rules
    interval: 30s
    rules:
      # Pre-compute per-handler request rate
      - record: job:http_requests:rate5m
        expr: sum by (job, handler) (rate(http_requests_total[5m]))

      # Pre-compute error rate percentage
      - record: job:http_errors:ratio_rate5m
        expr: |
          sum by (job) (rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum by (job) (rate(http_requests_total[5m]))

      # Pre-compute P95 latency per handler
      - record: job:http_duration_seconds:p95_5m
        expr: |
          histogram_quantile(0.95,
            sum by (job, handler, le) (
              rate(http_request_duration_seconds_bucket[5m])
            )
          )

      # Pre-compute P99 latency (global)
      - record: job:http_duration_seconds:p99_5m
        expr: |
          histogram_quantile(0.99,
            sum by (job, le) (
              rate(http_request_duration_seconds_bucket[5m])
            )
          )

  - name: node_recording_rules
    rules:
      # CPU usage by instance
      - record: instance:node_cpu:ratio_rate5m
        expr: |
          1 - avg by (instance) (
            rate(node_cpu_seconds_total{mode="idle"}[5m])
          )

      # Memory usage by instance
      - record: instance:node_memory:usage_ratio
        expr: |
          1 - (
            node_memory_MemAvailable_bytes
            /
            node_memory_MemTotal_bytes
          )</code></pre>
                </div>

                <!-- Prometheus Architecture -->
                <div class="subsection">
                    <h3 class="subsection-title">Prometheus Architecture</h3>

                    <p>Prometheus uses a <strong>pull-based</strong> model: it actively scrapes HTTP endpoints that expose metrics in the exposition format. This is fundamentally different from push-based systems like StatsD or Datadog Agent.</p>

<pre><code># Prometheus Architecture Overview
#
# +------------------+     scrape      +------------------+
# |   Application    | &lt;-------------- |    Prometheus    |
# |  /metrics        |    (HTTP GET)   |    Server        |
# |  endpoint        |                 |                  |
# +------------------+                 |  +------------+  |
#                                      |  |   TSDB     |  |
# +------------------+     scrape      |  | (storage)  |  |
# |   Node Exporter  | &lt;-------------- |  +------------+  |
# |  /metrics        |                 |                  |
# +------------------+                 |  +------------+  |
#                                      |  |  PromQL    |  |
# +------------------+     scrape      |  |  Engine    |  |
# |   cAdvisor       | &lt;-------------- |  +------------+  |
# |  /metrics        |                 |                  |
# +------------------+                 +-------+----------+
#                                              |
#                      +------------------+    |  evaluate rules
#                      |  Alertmanager    | &lt;--+
#                      |  (routing,       |    |  query
#                      |   dedup,         |    v
#                      |   silencing)     |  +------------------+
#                      +------------------+  |    Grafana       |
#                              |             |  (dashboards,    |
#                              v             |   visualization) |
#                      +------------------+  +------------------+
#                      | PagerDuty/Slack  |
#                      | Email/Webhook    |
#                      +------------------+</code></pre>

                    <div class="alert alert-info">
                        <div class="alert-title">Prometheus configuration: scrape targets</div>
                        <p>Targets are configured in <code>prometheus.yml</code> under <code>scrape_configs</code>. Use service discovery (Kubernetes, Consul, EC2) for dynamic environments instead of static target lists.</p>
                    </div>

<pre><code># prometheus.yml — minimal configuration
global:
  scrape_interval: 15s        # Default scrape frequency
  evaluation_interval: 15s    # Rule evaluation frequency
  scrape_timeout: 10s         # Per-scrape timeout

rule_files:
  - "rules/*.yaml"            # Recording & alerting rules

alerting:
  alertmanagers:
    - static_configs:
        - targets: ["alertmanager:9093"]

scrape_configs:
  # Prometheus scrapes itself
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  # Application metrics
  - job_name: "api-server"
    metrics_path: /metrics
    scrape_interval: 10s
    static_configs:
      - targets: ["api-server:8080"]
        labels:
          env: "production"
          team: "platform"

  # Node exporter (infrastructure)
  - job_name: "node"
    static_configs:
      - targets:
          - "node1:9100"
          - "node2:9100"
          - "node3:9100"

  # Kubernetes service discovery
  - job_name: "kubernetes-pods"
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)</code></pre>

                    <div class="alert alert-warning">
                        <div class="alert-title">Cardinality is the #1 Prometheus performance killer</div>
                        <p>Every unique combination of metric name + label values creates a separate time series. A metric with labels <code>{method, handler, status, instance}</code> across 100 instances with 50 handlers and 5 status codes = <strong>25,000 time series</strong> from one metric. Avoid high-cardinality labels like <code>user_id</code>, <code>request_id</code>, or <code>email</code>. Monitor cardinality with: <code>prometheus_tsdb_head_series</code>.</p>
                    </div>

                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 04 — LOGGING
         ============================================ -->

    <section id="logging" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">04</span>
                <div class="section-header-text">
                    <h2 class="section-title">Logging</h2>
                    <p class="section-description">Log levels, structured formats, correlation IDs, aggregation tools, and cost-efficient logging strategies for production systems.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Log Levels -->
                <div class="subsection">
                    <h3 class="subsection-title">Log Levels</h3>

                    <p>Log levels provide a severity hierarchy that allows you to filter and route messages based on importance. Every logging framework supports them, and consistent usage across services is critical for effective debugging and alerting.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Level</th>
                                    <th>Severity</th>
                                    <th>Description</th>
                                    <th>When to Use</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>DEBUG</code></td>
                                    <td><span class="badge badge-navy">Lowest</span></td>
                                    <td>Detailed diagnostic information for developers</td>
                                    <td>Variable values, function entry/exit, SQL queries, cache hits/misses</td>
                                </tr>
                                <tr>
                                    <td><code>INFO</code></td>
                                    <td><span class="badge badge-teal">Normal</span></td>
                                    <td>General operational events confirming things work as expected</td>
                                    <td>Server started, request completed, job scheduled, config loaded</td>
                                </tr>
                                <tr>
                                    <td><code>WARN</code></td>
                                    <td><span class="badge badge-amber">Elevated</span></td>
                                    <td>Potentially harmful situation that does not prevent operation</td>
                                    <td>Deprecated API call, retry attempt, approaching resource limit, fallback used</td>
                                </tr>
                                <tr>
                                    <td><code>ERROR</code></td>
                                    <td><span class="badge badge-clay">High</span></td>
                                    <td>An error event that might still allow the application to continue</td>
                                    <td>Failed request, database connection error, external API failure, unhandled exception</td>
                                </tr>
                                <tr>
                                    <td><code>FATAL</code></td>
                                    <td><span class="badge badge-clay">Critical</span></td>
                                    <td>Severe error that will likely cause the application to terminate</td>
                                    <td>Cannot bind port, out of memory, required config missing, data corruption detected</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="alert alert-info">
                        <div class="alert-title">Production log level guidance</div>
                        <p>In production, set the minimum log level to <strong>INFO</strong>. Use <strong>DEBUG</strong> only for targeted troubleshooting, ideally controlled per-service via a config flag or environment variable. Never leave DEBUG logging enabled permanently in production &mdash; it generates massive volumes and can expose sensitive data.</p>
                    </div>
                </div>

                <!-- Structured Logging -->
                <div class="subsection">
                    <h3 class="subsection-title">Structured Logging</h3>

                    <p>Structured logging outputs log entries as machine-parseable data (typically JSON) rather than free-form text. This is the foundation of modern observability &mdash; structured logs enable field-level filtering, aggregation, and correlation without fragile regex parsing.</p>

                    <h4 class="subsection-subtitle">Unstructured vs Structured</h4>

<pre><code># Unstructured — human-readable, machine-hostile
2024-03-15 14:23:45 ERROR Payment failed for user usr_8x7k2m order ord_3f9a1c: timeout after 30s

# Structured — machine-readable, queryable, correlatable
{"timestamp":"2024-03-15T14:23:45.123Z","level":"ERROR","service":"payment-api",
 "trace_id":"abc123def456","message":"Payment failed","user_id":"usr_8x7k2m",
 "order_id":"ord_3f9a1c","error":"timeout","duration_ms":30000}</code></pre>

                    <p><strong>Why structured logging matters:</strong></p>
                    <ul>
                        <li><strong>Queryable:</strong> Filter by any field (<code>level=ERROR AND service=payment-api</code>)</li>
                        <li><strong>Aggregatable:</strong> Count errors by type, service, or endpoint</li>
                        <li><strong>Correlatable:</strong> Join logs across services using <code>trace_id</code></li>
                        <li><strong>Parseable:</strong> No regex required &mdash; tools like Loki, Elasticsearch, and Splunk parse JSON natively</li>
                    </ul>

                    <h4 class="subsection-subtitle">Python &mdash; structlog</h4>

<pre><code>import structlog

# Configure structlog with JSON output
structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer(),
    ],
)

logger = structlog.get_logger()

# Basic structured log
logger.info("request_completed",
    method="GET",
    path="/api/users",
    status=200,
    duration_ms=45,
    user_id="usr_8x7k2m"
)
# Output: {"event":"request_completed","level":"info",
#   "timestamp":"2024-03-15T14:23:45.123Z","method":"GET",
#   "path":"/api/users","status":200,"duration_ms":45,
#   "user_id":"usr_8x7k2m"}

# Bind context that persists across log calls
logger = logger.bind(service="payment-api", env="production")
logger.error("payment_failed",
    order_id="ord_3f9a1c",
    error="gateway_timeout",
    retry_count=2
)</code></pre>

                    <h4 class="subsection-subtitle">Go &mdash; log/slog (stdlib)</h4>

<pre><code>package main

import (
    "log/slog"
    "os"
)

func main() {
    // JSON handler for structured output
    logger := slog.New(slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{
        Level: slog.LevelInfo,
    }))
    slog.SetDefault(logger)

    // Structured log with typed attributes
    slog.Info("request completed",
        slog.String("method", "GET"),
        slog.String("path", "/api/users"),
        slog.Int("status", 200),
        slog.Duration("duration", elapsed),
        slog.String("trace_id", traceID),
    )

    // Group related attributes
    slog.Error("payment failed",
        slog.Group("order",
            slog.String("id", "ord_3f9a1c"),
            slog.Float64("amount", 49.99),
        ),
        slog.String("error", "gateway_timeout"),
    )
}</code></pre>

                    <h4 class="subsection-subtitle">Node.js &mdash; pino</h4>

<pre><code>const pino = require('pino');

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  timestamp: pino.stdTimeFunctions.isoTime,
  formatters: {
    level: (label) => ({ level: label }),
  },
  // Redact sensitive fields
  redact: ['req.headers.authorization', 'body.password'],
});

// Structured log entry
logger.info({
  method: 'GET',
  path: '/api/users',
  status: 200,
  duration_ms: 45,
  trace_id: 'abc123def456',
}, 'request completed');

// Child logger with bound context
const reqLogger = logger.child({
  service: 'payment-api',
  request_id: 'req_xyz789',
});

reqLogger.error({
  order_id: 'ord_3f9a1c',
  error: 'gateway_timeout',
  retry_count: 2,
}, 'payment failed');</code></pre>

                    <h4 class="subsection-subtitle">Key Fields Every Log Should Have</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Field</th>
                                    <th>Type</th>
                                    <th>Purpose</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>timestamp</code></td>
                                    <td>ISO 8601</td>
                                    <td>When the event occurred (UTC)</td>
                                    <td><code>2024-03-15T14:23:45.123Z</code></td>
                                </tr>
                                <tr>
                                    <td><code>level</code></td>
                                    <td>string</td>
                                    <td>Severity of the event</td>
                                    <td><code>info</code>, <code>error</code>, <code>warn</code></td>
                                </tr>
                                <tr>
                                    <td><code>service</code></td>
                                    <td>string</td>
                                    <td>Which service emitted the log</td>
                                    <td><code>payment-api</code></td>
                                </tr>
                                <tr>
                                    <td><code>trace_id</code></td>
                                    <td>string</td>
                                    <td>Correlation with distributed traces</td>
                                    <td><code>abc123def456</code></td>
                                </tr>
                                <tr>
                                    <td><code>message</code></td>
                                    <td>string</td>
                                    <td>Human-readable event description</td>
                                    <td><code>Payment processing failed</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- Correlation IDs -->
                <div class="subsection">
                    <h3 class="subsection-title">Correlation IDs</h3>

                    <p>A <strong>correlation ID</strong> (also called a request ID) is a unique identifier that follows a request across every service it touches. It is the single most important field for debugging distributed systems &mdash; it lets you find every log entry, span, and metric related to one user action.</p>

                    <p><strong>How it works:</strong></p>
                    <ul>
                        <li>The first service (API gateway or load balancer) generates a unique ID</li>
                        <li>The ID is passed via HTTP headers to every downstream service</li>
                        <li>Every service includes the ID in all log entries and spans</li>
                        <li>Common header names: <code>X-Correlation-ID</code>, <code>X-Request-ID</code>, <code>traceparent</code> (W3C)</li>
                    </ul>

                    <h4 class="subsection-subtitle">Middleware Example (Python / Flask)</h4>

<pre><code>import uuid
from flask import Flask, request, g

app = Flask(__name__)

@app.before_request
def set_correlation_id():
    """Extract or generate correlation ID for every request."""
    # Check incoming headers (may come from upstream service)
    correlation_id = (
        request.headers.get('X-Correlation-ID') or
        request.headers.get('X-Request-ID') or
        str(uuid.uuid4())
    )
    g.correlation_id = correlation_id

@app.after_request
def add_correlation_header(response):
    """Include correlation ID in response headers."""
    response.headers['X-Correlation-ID'] = g.correlation_id
    return response

# When calling downstream services, propagate the ID
import requests as http_client

def call_downstream(url, data):
    return http_client.post(url, json=data, headers={
        'X-Correlation-ID': g.correlation_id,
        'Content-Type': 'application/json',
    })</code></pre>

                    <h4 class="subsection-subtitle">Request Flow with Correlation ID</h4>

<pre><code># Correlation ID propagation across 3 services
#
# correlation_id = "req-7a3f-2b1c-4d5e"
#
# +----------+          +------------+          +-------------+
# |  Client  |  ------> | API Gateway|  ------> | Order Svc   |
# |          |          |            |          |             |
# |          |          | Generates: |          | Receives:   |
# |          |          | X-Corr-ID  |          | X-Corr-ID   |
# +----------+          +-----+------+          +------+------+
#                             |                        |
#                             |                        v
#                             |                 +-------------+
#                             |                 | Payment Svc |
#                             |                 |             |
#                             |                 | Receives:   |
#                             |                 | X-Corr-ID   |
#                             |                 +-------------+
#
# All 3 services log with the same correlation_id:
#
# [API Gateway]  {"correlation_id":"req-7a3f-2b1c-4d5e","msg":"request received"}
# [Order Svc]    {"correlation_id":"req-7a3f-2b1c-4d5e","msg":"order created"}
# [Payment Svc]  {"correlation_id":"req-7a3f-2b1c-4d5e","msg":"payment processed"}
#
# Query all logs for this request:
#   {job=~".+"} | json | correlation_id = "req-7a3f-2b1c-4d5e"</code></pre>
                </div>

                <!-- Log Aggregation Tools -->
                <div class="subsection">
                    <h3 class="subsection-title">Log Aggregation Tools</h3>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Label-based</span>
                                Grafana Loki
                            </div>
                            <p class="command-card-desc">Like Prometheus, but for logs. Indexes <strong>labels only</strong>, not log content, making it dramatically cheaper to operate at scale. Uses LogQL for queries.</p>
                            <pre><code># Key characteristics
# - Label-based indexing (not full-text)
# - LogQL query language
# - Object storage backend (S3, GCS)
# - 10-100x cheaper than Elasticsearch
# - Native Grafana integration

# Storage: ~$0.02/GB/month (S3)
# vs Elasticsearch: ~$0.50/GB/month</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Full-text</span>
                                ELK Stack
                            </div>
                            <p class="command-card-desc"><strong>Elasticsearch + Logstash + Kibana.</strong> Full-text search and analytics engine. Powerful but resource-hungry. Best when you need complex text queries.</p>
                            <pre><code># Components
# Elasticsearch — search &amp; storage
# Logstash — ingestion &amp; transform
# Kibana — visualization &amp; dashboards
# Beats — lightweight log shippers

# Strengths: full-text search,
#   complex aggregations, mature
# Weakness: high memory/storage cost</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Forwarding</span>
                                Fluentd / Fluent Bit
                            </div>
                            <p class="command-card-desc">Unified log forwarding and processing layer. Collects from multiple sources, transforms, and routes to any destination. Fluent Bit is the lightweight variant.</p>
                            <pre><code># Fluentd — full-featured, Ruby
# Fluent Bit — lightweight, C
#
# Input plugins: tail, syslog,
#   docker, kubernetes
# Output plugins: Loki, ES, S3,
#   Kafka, Datadog, CloudWatch
#
# Use Fluent Bit as a DaemonSet
# in Kubernetes for log collection</code></pre>
                        </div>
                    </div>
                </div>

                <!-- LogQL Examples -->
                <div class="subsection">
                    <h3 class="subsection-title">LogQL Examples</h3>

                    <p>LogQL is Grafana Loki's query language. It uses the same label matching syntax as PromQL, combined with pipeline operators for filtering, parsing, and aggregating log content.</p>

<pre><code># --- Basic label matching ---
# All logs from the api job
{job="api"}

# Multiple label matchers
{job="api", env="production", level="error"}

# Regex label matching
{job=~"api|payment", namespace="prod"}

# --- Filter by content ---
# Lines containing "error"
{job="api"} |= "error"

# Lines NOT containing "healthcheck"
{job="api"} != "healthcheck"

# Regex filter
{job="api"} |~ "status=(4|5)\\d{2}"

# --- JSON parsing ---
# Parse JSON and filter by field
{job="api"} | json | status >= 500

# Extract specific fields
{job="api"} | json | line_format "{{.method}} {{.path}} {{.status}}"

# Filter parsed fields
{job="api"} | json | duration_ms > 1000 | level = "error"

# --- Rate queries (metric from logs) ---
# Error rate per second over 5 minutes
rate({job="api"} |= "error" [5m])

# Errors per minute by service
sum by (service) (
  rate({job=~".+"} | json | level = "error" [5m])
) * 60

# --- Top errors ---
# Top 10 error messages by frequency
topk(10,
  sum by (error) (
    count_over_time({job="api"} | json | level = "error" [1h])
  )
)

# --- Bytes rate (log volume) ---
# Bytes per second by job
bytes_rate({job="api"} [5m])

# --- Quantile from logs ---
# P95 duration from structured logs
quantile_over_time(0.95,
  {job="api"} | json | unwrap duration_ms [5m]
) by (handler)</code></pre>
                </div>

                <!-- Log Sampling & Cost Control -->
                <div class="subsection">
                    <h3 class="subsection-title">Log Sampling &amp; Cost Control</h3>

                    <div class="alert alert-warning">
                        <div class="alert-title">Logging everything in production can cost more than your infrastructure</div>
                        <p>A single service logging 1,000 req/s at 1 KB/log generates <strong>86 GB/day</strong>. Across 20 services, that is <strong>1.7 TB/day</strong> before ingestion overhead. At $0.50/GB for Elasticsearch, that is <strong>$850/day</strong> just for log storage. Sampling and filtering are not optional &mdash; they are economic necessities.</p>
                    </div>

                    <p><strong>Cost control strategies:</strong></p>
                    <ul>
                        <li><strong>Sample DEBUG logs:</strong> Log 1-10% of DEBUG messages using a token bucket or probabilistic sampler</li>
                        <li><strong>Always keep ERROR and FATAL:</strong> Never sample error-level logs &mdash; these are your lifeline during incidents</li>
                        <li><strong>Rate-limit per source:</strong> Cap log throughput per service/pod to prevent noisy neighbor problems</li>
                        <li><strong>Drop health check logs:</strong> Filter out high-frequency, low-value logs like <code>/healthz</code> and <code>/readyz</code></li>
                        <li><strong>Use log levels aggressively:</strong> Set production minimum to INFO, enable DEBUG per-service as needed</li>
                        <li><strong>Tiered retention:</strong> Keep hot logs for 7 days, warm for 30 days, cold (S3) for 90 days</li>
                    </ul>

                    <h4 class="subsection-subtitle">Token Bucket Sampling Pattern</h4>

<pre><code># Token bucket rate limiter for log sampling
# Allows burst logging but caps sustained rate

import time
import threading

class LogSampler:
    """Token bucket sampler: allows `rate` logs/sec with
    burst capacity of `bucket_size`."""

    def __init__(self, rate=10, bucket_size=50):
        self.rate = rate            # tokens per second
        self.bucket_size = bucket_size
        self.tokens = bucket_size   # start full
        self.last_refill = time.monotonic()
        self.lock = threading.Lock()

    def should_log(self, level: str) -> bool:
        # Always log ERROR and above
        if level in ("ERROR", "FATAL", "CRITICAL"):
            return True

        with self.lock:
            now = time.monotonic()
            elapsed = now - self.last_refill
            self.tokens = min(
                self.bucket_size,
                self.tokens + elapsed * self.rate
            )
            self.last_refill = now

            if self.tokens >= 1:
                self.tokens -= 1
                return True
            return False

# Usage
sampler = LogSampler(rate=10, bucket_size=50)

if sampler.should_log("DEBUG"):
    logger.debug("cache_miss", key=cache_key)

# ERROR always passes through
if sampler.should_log("ERROR"):
    logger.error("payment_failed", order_id=order_id)</code></pre>

                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 05 — DISTRIBUTED TRACING
         ============================================ -->

    <section id="tracing" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">05</span>
                <div class="section-header-text">
                    <h2 class="section-title">Distributed Tracing</h2>
                    <p class="section-description">Trace structure, span anatomy, instrumentation patterns, sampling strategies, and tracing backend comparisons.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Core Concepts -->
                <div class="subsection">
                    <h3 class="subsection-title">Core Concepts</h3>

                    <p>Distributed tracing captures the end-to-end path of a request as it flows through multiple services. Each trace tells the story of one user action &mdash; which services were involved, how long each took, and where failures occurred.</p>

                    <ul>
                        <li><strong>Trace:</strong> The complete end-to-end request path, identified by a unique <code>trace_id</code>. A trace is a directed acyclic graph of spans.</li>
                        <li><strong>Span:</strong> A single unit of work within a trace &mdash; an HTTP call, a database query, a function invocation. Each span has a start time, duration, and status.</li>
                        <li><strong>Context Propagation:</strong> The mechanism by which trace context (<code>trace_id</code>, <code>span_id</code>) passes between services, typically via HTTP headers like <code>traceparent</code> (W3C) or <code>b3</code> (Zipkin).</li>
                        <li><strong>Baggage:</strong> Key-value pairs that travel with the trace context across all service boundaries. Useful for propagating user IDs, tenant IDs, or feature flags without modifying service interfaces.</li>
                    </ul>

                    <h4 class="subsection-subtitle">Trace Structure</h4>

                    <div class="trace-diagram">
<span class="span-id">Trace ID: abc123</span>

<span class="span-name">API Gateway</span>          <span class="span-bar">|============================|</span>         <span class="span-time">120ms</span>
<span class="span-name">  Auth Service</span>        <span class="span-bar">|===|</span>                                       <span class="span-time"> 15ms</span>
<span class="span-name">  Order Service</span>       <span class="span-bar">      |=====================|</span>              <span class="span-time"> 90ms</span>
<span class="span-name">    Database Query</span>    <span class="span-bar">        |========|</span>                         <span class="span-time"> 25ms</span>
<span class="span-name">    Payment Service</span>   <span class="span-bar">                  |============|</span>          <span class="span-time"> 50ms</span>
<span class="span-name">      Stripe API</span>      <span class="span-bar">                    |=========|</span>           <span class="span-time"> 40ms</span>
                    </div>

                    <p>Each span is a child of the span that initiated it. The root span (API Gateway) encompasses the entire request. Child spans show the work breakdown, revealing where time is actually spent.</p>
                </div>

                <!-- Span Anatomy -->
                <div class="subsection">
                    <h3 class="subsection-title">Span Anatomy</h3>

                    <p>Every span carries a set of standard attributes that describe the work it represents. Understanding these fields is essential for effective trace analysis and custom instrumentation.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Attribute</th>
                                    <th>Type</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>trace_id</code></td>
                                    <td>string (128-bit)</td>
                                    <td>Unique identifier for the entire trace</td>
                                    <td><code>4bf92f3577b34da6a3ce929d0e0e4736</code></td>
                                </tr>
                                <tr>
                                    <td><code>span_id</code></td>
                                    <td>string (64-bit)</td>
                                    <td>Unique identifier for this span</td>
                                    <td><code>00f067aa0ba902b7</code></td>
                                </tr>
                                <tr>
                                    <td><code>parent_span_id</code></td>
                                    <td>string (64-bit)</td>
                                    <td>ID of the parent span (empty for root)</td>
                                    <td><code>a3ce929d0e0e4736</code></td>
                                </tr>
                                <tr>
                                    <td><code>operation_name</code></td>
                                    <td>string</td>
                                    <td>Human-readable name of the operation</td>
                                    <td><code>HTTP GET /api/orders</code></td>
                                </tr>
                                <tr>
                                    <td><code>start_time</code></td>
                                    <td>timestamp</td>
                                    <td>When the span started (nanosecond precision)</td>
                                    <td><code>2024-03-15T14:23:45.123456789Z</code></td>
                                </tr>
                                <tr>
                                    <td><code>duration</code></td>
                                    <td>nanoseconds</td>
                                    <td>How long the operation took</td>
                                    <td><code>45200000</code> (45.2ms)</td>
                                </tr>
                                <tr>
                                    <td><code>status</code></td>
                                    <td>enum</td>
                                    <td>OK, ERROR, or UNSET</td>
                                    <td><code>ERROR</code></td>
                                </tr>
                                <tr>
                                    <td><code>attributes</code></td>
                                    <td>key-value map</td>
                                    <td>Custom metadata on the span</td>
                                    <td><code>http.method=GET, http.status_code=200</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Example Span (JSON)</h4>

<pre><code>{
  "traceId": "4bf92f3577b34da6a3ce929d0e0e4736",
  "spanId": "00f067aa0ba902b7",
  "parentSpanId": "a3ce929d0e0e4736",
  "operationName": "HTTP GET /api/orders/ord_3f9a1c",
  "startTime": "2024-03-15T14:23:45.123456789Z",
  "duration": 45200000,
  "status": { "code": "OK" },
  "attributes": {
    "http.method": "GET",
    "http.url": "/api/orders/ord_3f9a1c",
    "http.status_code": 200,
    "http.response_content_length": 1284,
    "service.name": "order-service",
    "service.version": "1.4.2",
    "deployment.environment": "production"
  },
  "events": [
    {
      "name": "cache_miss",
      "timestamp": "2024-03-15T14:23:45.125000000Z",
      "attributes": { "cache.key": "order:ord_3f9a1c" }
    }
  ],
  "links": []
}</code></pre>
                </div>

                <!-- Instrumentation -->
                <div class="subsection">
                    <h3 class="subsection-title">Instrumentation</h3>

                    <p>Instrumentation is the process of adding tracing code to your application. OpenTelemetry provides two approaches: <strong>auto-instrumentation</strong> (zero-code, uses agents/hooks) and <strong>manual instrumentation</strong> (explicit span creation in your code).</p>

                    <h4 class="subsection-subtitle">Auto vs Manual Instrumentation</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Auto-Instrumentation</th>
                                    <th>Manual Instrumentation</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Setup effort</strong></td>
                                    <td><span class="badge badge-moss">Low</span> Install agent/package</td>
                                    <td><span class="badge badge-amber">Medium</span> Add code to each operation</td>
                                </tr>
                                <tr>
                                    <td><strong>Coverage</strong></td>
                                    <td>HTTP, gRPC, database clients, messaging</td>
                                    <td>Any code path you choose</td>
                                </tr>
                                <tr>
                                    <td><strong>Custom attributes</strong></td>
                                    <td>Limited (generic HTTP/DB attributes)</td>
                                    <td>Full control (business-specific data)</td>
                                </tr>
                                <tr>
                                    <td><strong>Maintenance</strong></td>
                                    <td>Automatic updates with library versions</td>
                                    <td>Must maintain with code changes</td>
                                </tr>
                                <tr>
                                    <td><strong>Best for</strong></td>
                                    <td>Quick start, framework-level visibility</td>
                                    <td>Business logic, custom operations</td>
                                </tr>
                                <tr>
                                    <td><strong>Recommendation</strong></td>
                                    <td colspan="2">Use <strong>both</strong>: auto-instrumentation as the base, manual spans for business-critical paths</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Python &mdash; OpenTelemetry Manual Spans</h4>

<pre><code>from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Setup
provider = TracerProvider()
provider.add_span_processor(
    BatchSpanProcessor(OTLPSpanExporter(endpoint="localhost:4317"))
)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer("order-service")

def process_order(order_id: str, user_id: str):
    # Create a span for this operation
    with tracer.start_as_current_span("process-order") as span:
        span.set_attribute("order.id", order_id)
        span.set_attribute("user.id", user_id)

        # Child span for database lookup
        with tracer.start_as_current_span("db-lookup") as db_span:
            db_span.set_attribute("db.system", "postgresql")
            db_span.set_attribute("db.statement", "SELECT * FROM orders WHERE id = $1")
            order = db.fetch_order(order_id)

        # Child span for payment
        with tracer.start_as_current_span("charge-payment") as pay_span:
            pay_span.set_attribute("payment.gateway", "stripe")
            pay_span.set_attribute("payment.amount", order.total)
            try:
                result = payment_gateway.charge(order)
                pay_span.set_attribute("payment.status", "success")
            except PaymentError as e:
                pay_span.set_status(trace.StatusCode.ERROR, str(e))
                pay_span.record_exception(e)
                raise

        span.set_attribute("order.status", "completed")
        return order</code></pre>

                    <h4 class="subsection-subtitle">Context Propagation Between Services</h4>

<pre><code># Service A — outgoing HTTP call (inject context)
from opentelemetry.propagate import inject
import requests

def call_payment_service(order):
    headers = {}
    inject(headers)  # Injects traceparent header
    # headers now contains:
    # {"traceparent": "00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01"}

    response = requests.post(
        "http://payment-service/charge",
        json=order.to_dict(),
        headers=headers,
    )
    return response.json()


# Service B — incoming HTTP request (extract context)
from opentelemetry.propagate import extract
from flask import request

@app.route("/charge", methods=["POST"])
def handle_charge():
    # Extract trace context from incoming headers
    context = extract(request.headers)

    # Start a new span linked to the parent trace
    with tracer.start_as_current_span(
        "handle-charge",
        context=context,
    ) as span:
        span.set_attribute("payment.method", "credit_card")
        # This span is now a child of the span in Service A
        result = process_charge(request.json)
        return jsonify(result)</code></pre>
                </div>

                <!-- Sampling Strategies -->
                <div class="subsection">
                    <h3 class="subsection-title">Sampling Strategies</h3>

                    <p>Tracing every request in a high-traffic system is prohibitively expensive. Sampling reduces volume while preserving the traces that matter most &mdash; errors, slow requests, and important business transactions.</p>

                    <ul>
                        <li><strong>Head-based sampling:</strong> The decision to sample is made at the very start of a trace (at the root span). Simple, low overhead, but may miss interesting traces because the decision is made before the outcome is known.</li>
                        <li><strong>Tail-based sampling:</strong> The decision is made after the trace is complete, at the collector level. This allows keeping all error traces and slow traces, but requires buffering complete traces in memory, increasing collector overhead.</li>
                        <li><strong>Combined approach:</strong> Head-sample at a base rate (e.g., 10%), then tail-sample at the collector to ensure all errors and slow traces are retained.</li>
                    </ul>

                    <h4 class="subsection-subtitle">Sampling Strategy Comparison</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Strategy</th>
                                    <th>Decision Point</th>
                                    <th>Overhead</th>
                                    <th>Error Coverage</th>
                                    <th>Best For</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Head-based</strong></td>
                                    <td>Trace start (root span)</td>
                                    <td><span class="badge badge-moss">Low</span></td>
                                    <td><span class="badge badge-amber">Partial</span> (random)</td>
                                    <td>High-traffic, cost-sensitive systems</td>
                                </tr>
                                <tr>
                                    <td><strong>Tail-based</strong></td>
                                    <td>After trace completes (collector)</td>
                                    <td><span class="badge badge-clay">High</span> (memory)</td>
                                    <td><span class="badge badge-moss">Full</span></td>
                                    <td>When every error trace must be captured</td>
                                </tr>
                                <tr>
                                    <td><strong>Combined</strong></td>
                                    <td>Head first, tail at collector</td>
                                    <td><span class="badge badge-amber">Medium</span></td>
                                    <td><span class="badge badge-moss">Full</span></td>
                                    <td>Best balance of cost and coverage</td>
                                </tr>
                                <tr>
                                    <td><strong>Rule-based</strong></td>
                                    <td>Per-service rules</td>
                                    <td><span class="badge badge-amber">Medium</span></td>
                                    <td><span class="badge badge-teal">Configurable</span></td>
                                    <td>Different rates for different endpoints</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">OTel Collector Tail Sampling Configuration</h4>

<pre><code># otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  tail_sampling:
    decision_wait: 10s           # Wait time for complete trace
    num_traces: 100000           # Max traces in memory
    expected_new_traces_per_sec: 1000
    policies:
      # Always keep error traces
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]

      # Always keep slow traces (> 1 second)
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000

      # Sample 10% of remaining traces
      - name: probabilistic
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

      # Always trace specific endpoints
      - name: critical-endpoints
        type: string_attribute
        string_attribute:
          key: http.url
          values: ["/api/checkout", "/api/payment"]

  batch:
    timeout: 5s
    send_batch_size: 1000

exporters:
  otlp:
    endpoint: tempo:4317
    tls:
      insecure: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [tail_sampling, batch]
      exporters: [otlp]</code></pre>
                </div>

                <!-- Tracing Tools -->
                <div class="subsection">
                    <h3 class="subsection-title">Tracing Tools</h3>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">CNCF</span>
                                Jaeger
                            </div>
                            <p class="command-card-desc">End-to-end distributed tracing platform originally built by Uber. CNCF graduated project. Rich UI with dependency graphs and trace comparison.</p>
                            <pre><code># Run Jaeger all-in-one (dev)
docker run -d --name jaeger \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest

# UI: http://localhost:16686
# Accepts: OTLP, Jaeger, Zipkin</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Grafana</span>
                                Grafana Tempo
                            </div>
                            <p class="command-card-desc">High-scale trace backend with minimal indexing. Stores traces in object storage (S3/GCS). Integrates natively with Grafana, Loki, and Mimir.</p>
                            <pre><code># Key advantages
# - Label-based, no full-text index
# - Object storage = low cost
# - TraceQL query language
# - Exemplar links from metrics
# - Logs-to-traces integration

# TraceQL example
{ span.http.status_code >= 500
  &amp;&amp; resource.service.name = "api" }</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Lightweight</span>
                                Zipkin
                            </div>
                            <p class="command-card-desc">One of the original distributed tracing systems, inspired by Google Dapper. Lightweight, simple setup, good for smaller deployments.</p>
                            <pre><code># Run Zipkin (dev)
docker run -d -p 9411:9411 \
  openzipkin/zipkin

# UI: http://localhost:9411
# Accepts: Zipkin B3, OTLP
# Storage: in-memory, MySQL,
#   Cassandra, Elasticsearch</code></pre>
                        </div>
                    </div>

                    <h4 class="subsection-subtitle">Tracing Backend Comparison</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>Jaeger</th>
                                    <th>Grafana Tempo</th>
                                    <th>Zipkin</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Query Language</strong></td>
                                    <td>UI-based search</td>
                                    <td>TraceQL</td>
                                    <td>UI-based search</td>
                                </tr>
                                <tr>
                                    <td><strong>Storage</strong></td>
                                    <td>Elasticsearch, Cassandra, Kafka</td>
                                    <td>Object storage (S3, GCS, Azure)</td>
                                    <td>In-memory, MySQL, Cassandra, ES</td>
                                </tr>
                                <tr>
                                    <td><strong>Scale</strong></td>
                                    <td><span class="badge badge-moss">Large</span></td>
                                    <td><span class="badge badge-teal">Very large</span></td>
                                    <td><span class="badge badge-amber">Small-Medium</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Cost at scale</strong></td>
                                    <td>Medium (requires ES/Cassandra)</td>
                                    <td>Low (object storage)</td>
                                    <td>Low (simple storage)</td>
                                </tr>
                                <tr>
                                    <td><strong>Grafana integration</strong></td>
                                    <td>Plugin</td>
                                    <td>Native</td>
                                    <td>Plugin</td>
                                </tr>
                                <tr>
                                    <td><strong>Best for</strong></td>
                                    <td>Teams wanting rich UI and dependency maps</td>
                                    <td>Grafana stack users at scale</td>
                                    <td>Small teams wanting simplicity</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 06 — ALERTING & ON-CALL
         ============================================ -->

    <section id="alerting" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">06</span>
                <div class="section-header-text">
                    <h2 class="section-title">Alerting &amp; On-Call</h2>
                    <p class="section-description">Prometheus alert rules, Alertmanager routing, on-call best practices, runbook templates, and common anti-patterns to avoid.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Prometheus Alert Rules -->
                <div class="subsection">
                    <h3 class="subsection-title">Prometheus Alert Rules</h3>

                    <p>Alert rules are PromQL expressions evaluated at regular intervals. When an expression returns results and the condition persists for the <code>for</code> duration, the alert fires and is sent to Alertmanager.</p>

<pre><code># alerting-rules.yaml
groups:
  - name: service_alerts
    interval: 30s
    rules:

      # --- High Latency ---
      # P95 latency exceeds 500ms for 5 minutes
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High P95 latency on {{ $labels.job }}"
          description: "P95 latency is {{ $value | humanizeDuration }} (threshold: 500ms)"
          runbook: "https://wiki.internal/runbooks/high-latency"

      # --- High Error Rate ---
      # Error rate exceeds 1% for 5 minutes
      - alert: HighErrorRate
        expr: |
          sum by (job) (rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum by (job) (rate(http_requests_total[5m]))
          * 100 > 1
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "Error rate is {{ $value | printf \"%.2f\" }}% (threshold: 1%)"
          runbook: "https://wiki.internal/runbooks/high-error-rate"

      # --- High CPU ---
      # CPU usage above 80% for 10 minutes
      - alert: HighCPU
        expr: |
          100 - (avg by (instance) (
            rate(node_cpu_seconds_total{mode="idle"}[5m])
          ) * 100) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | printf \"%.1f\" }}%"
          runbook: "https://wiki.internal/runbooks/high-cpu"

      # --- Disk Will Fill ---
      # Linear prediction shows disk full within 4 hours
      - alert: DiskWillFill
        expr: |
          predict_linear(
            node_filesystem_avail_bytes{mountpoint="/"}[6h], 4 * 3600
          ) < 0
        for: 30m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Disk on {{ $labels.instance }} predicted to fill within 4h"
          description: "Current available: {{ $value | humanize1024 }}B"
          runbook: "https://wiki.internal/runbooks/disk-full"

      # --- Traffic Drop ---
      # 50% decrease in traffic compared to same time yesterday
      - alert: TrafficDrop
        expr: |
          sum(rate(http_requests_total[5m]))
          /
          sum(rate(http_requests_total[5m] offset 1d))
          < 0.5
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Significant traffic drop detected"
          description: "Current traffic is {{ $value | printf \"%.0f\" }}% of yesterday"
          runbook: "https://wiki.internal/runbooks/traffic-drop"</code></pre>
                </div>

                <!-- Alertmanager Configuration -->
                <div class="subsection">
                    <h3 class="subsection-title">Alertmanager Configuration</h3>

                    <p>Alertmanager handles alert <strong>deduplication</strong>, <strong>grouping</strong>, <strong>routing</strong>, <strong>silencing</strong>, and <strong>inhibition</strong>. Its routing tree determines which alerts go to which receivers based on label matching.</p>

<pre><code># alertmanager.yml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.internal:587'
  smtp_from: 'alerts@company.com'
  slack_api_url: 'https://hooks.slack.com/services/T00/B00/xxxxx'

# Inhibition — suppress less severe alerts when critical is firing
inhibit_rules:
  - source_matchers:
      - severity = critical
    target_matchers:
      - severity = warning
    equal: ['job', 'instance']

# Routing tree — match alerts to receivers
route:
  receiver: default-email          # Fallback receiver
  group_by: ['alertname', 'job']   # Group related alerts
  group_wait: 30s                  # Wait before first notification
  group_interval: 5m               # Wait between grouped notifications
  repeat_interval: 4h              # Re-notify after this interval

  routes:
    # Critical alerts → PagerDuty (immediate)
    - matchers:
        - severity = critical
      receiver: pagerduty-critical
      group_wait: 10s
      repeat_interval: 1h
      continue: false

    # Warning alerts → Slack channel
    - matchers:
        - severity = warning
      receiver: slack-warnings
      group_wait: 1m
      repeat_interval: 4h

    # Infrastructure team alerts
    - matchers:
        - team = infrastructure
      receiver: slack-infra
      routes:
        - matchers:
            - severity = critical
          receiver: pagerduty-infra

# Receivers
receivers:
  - name: default-email
    email_configs:
      - to: 'oncall@company.com'
        send_resolved: true

  - name: pagerduty-critical
    pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_KEY'
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          description: '{{ .CommonAnnotations.description }}'
          runbook: '{{ .CommonAnnotations.runbook }}'

  - name: slack-warnings
    slack_configs:
      - channel: '#alerts-warning'
        send_resolved: true
        title: '{{ .CommonLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
        actions:
          - type: button
            text: 'Runbook'
            url: '{{ .CommonAnnotations.runbook }}'
          - type: button
            text: 'Silence'
            url: '{{ .ExternalURL }}/#/silences/new?filter=%7B'

  - name: slack-infra
    slack_configs:
      - channel: '#alerts-infra'
        send_resolved: true

  - name: pagerduty-infra
    pagerduty_configs:
      - routing_key: 'YOUR_INFRA_PD_KEY'</code></pre>

                    <div class="alert alert-info">
                        <div class="alert-title">Routing tree evaluation</div>
                        <p>Alertmanager evaluates routes top-to-bottom. The first matching route wins, unless <code>continue: true</code> is set, which allows an alert to match multiple routes. Use <code>group_by</code> to aggregate related alerts into a single notification &mdash; grouping by <code>[alertname, job]</code> means all instances of the same alert for the same job arrive as one message.</p>
                    </div>
                </div>

                <!-- On-Call Best Practices -->
                <div class="subsection">
                    <h3 class="subsection-title">On-Call Best Practices</h3>

                    <h4 class="subsection-subtitle">Escalation Policies</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Level</th>
                                    <th>Responder</th>
                                    <th>Timeout</th>
                                    <th>Action</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><span class="badge badge-teal">L1</span></td>
                                    <td>Primary on-call engineer</td>
                                    <td>5 minutes</td>
                                    <td>Acknowledge and begin investigation</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-amber">L2</span></td>
                                    <td>Secondary on-call engineer</td>
                                    <td>10 minutes</td>
                                    <td>Escalated if L1 does not acknowledge</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-clay">L3</span></td>
                                    <td>Engineering manager / team lead</td>
                                    <td>15 minutes</td>
                                    <td>Escalated if L2 does not acknowledge</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-clay">L4</span></td>
                                    <td>VP of Engineering / incident commander</td>
                                    <td>30 minutes</td>
                                    <td>Full incident response mobilized</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Alert Fatigue Reduction &mdash; 6 Strategies</h4>

                    <ol>
                        <li><strong>Set appropriate thresholds with <code>for</code> clause:</strong> A 30-second CPU spike is noise. Require the condition to persist for 5-10 minutes before alerting. The <code>for</code> clause eliminates transient spikes and flapping.</li>
                        <li><strong>Use inhibition rules:</strong> When a database is down, suppress all downstream service alerts. Inhibition rules in Alertmanager prevent alert storms from a single root cause.</li>
                        <li><strong>Aggregate related alerts:</strong> Group by <code>alertname</code> and <code>job</code> so 50 pod restarts arrive as one notification, not 50 separate pages.</li>
                        <li><strong>Regular alert review:</strong> Schedule monthly alert audits. Delete alerts that never fire, tune thresholds on alerts that fire too often, and improve runbooks for alerts that take too long to resolve.</li>
                        <li><strong>Every alert must be actionable:</strong> If the response to an alert is "ignore it" or "wait and see," delete the alert or change it to a warning. Every page should require immediate human action.</li>
                        <li><strong>Progressive severity:</strong> Start with a Slack warning. If the condition worsens or persists, escalate to a page. Not everything needs to wake someone at 3 AM.</li>
                    </ol>
                </div>

                <!-- Runbook Template -->
                <div class="subsection">
                    <h3 class="subsection-title">Runbook Template</h3>

                    <p>Every alert should link to a runbook. A good runbook lets any on-call engineer &mdash; even one unfamiliar with the service &mdash; diagnose and resolve the issue. Keep runbooks in version control alongside alert rules.</p>

<pre><code># ============================================
# RUNBOOK: HighErrorRate
# ============================================
# Alert:     HighErrorRate
# Severity:  critical
# Team:      platform
# Updated:   2024-03-15
# ============================================

## Summary
The 5xx error rate for the API service has exceeded 1%
for more than 5 minutes. This alert indicates a significant
number of user-facing requests are failing.

## Impact
- Users are experiencing errors on API calls
- Affected endpoints may be returning 500/502/503 errors
- Business transactions (orders, payments) may be failing

## Investigation Steps

### 1. Check the error rate dashboard
  Open: https://grafana.internal/d/api-errors

### 2. Identify which endpoints are failing
  Query:
    sum by (handler, status) (
      rate(http_requests_total{status=~"5.."}[5m])
    )

### 3. Check recent deployments
  kubectl rollout history deployment/api-server -n production

### 4. Check downstream dependencies
  # Database
  kubectl exec -it postgres-0 -- pg_isready
  # Redis
  redis-cli -h redis.internal ping
  # External APIs
  curl -s https://api.stripe.com/v1/health

### 5. Check application logs
  {job="api-server"} | json | level = "error" | line_format
    "{{.timestamp}} {{.error}} {{.path}}"

### 6. Check resource utilization
  # CPU, memory, disk on API pods
  kubectl top pods -n production -l app=api-server

## Resolution Steps

### If caused by a bad deployment:
  kubectl rollout undo deployment/api-server -n production

### If caused by database issues:
  # Check connection pool
  # Check slow queries
  # Check disk space on DB server

### If caused by downstream service failure:
  # Enable circuit breaker / fallback
  # Scale up affected service
  # Contact responsible team

## Escalation Path
1. Primary on-call (platform team)
2. Service owner: @jane-doe
3. Database team: #db-oncall (if DB-related)
4. Infrastructure: #infra-oncall (if resource-related)

## Related Alerts
- HighLatency (often fires alongside this alert)
- DatabaseConnectionPoolExhausted
- DownstreamServiceTimeout</code></pre>
                </div>

                <!-- Alert Anti-Patterns -->
                <div class="subsection">
                    <h3 class="subsection-title">Alert Anti-Patterns</h3>

                    <div class="alert alert-warning">
                        <div class="alert-title">Common alerting mistakes that cause on-call burnout</div>
                        <ul>
                            <li><strong>Alerting on symptoms instead of SLOs:</strong> Alert on "error budget burn rate > 2x" instead of individual error count thresholds. SLO-based alerts directly reflect user impact and are more meaningful.</li>
                            <li><strong>No <code>for</code> clause (flapping alerts):</strong> Without a <code>for</code> duration, alerts fire on momentary spikes and resolve immediately, creating a stream of fire-then-resolve notifications that erode trust.</li>
                            <li><strong>Missing runbook links:</strong> An alert without a runbook forces the on-call engineer to reverse-engineer the problem from scratch at 3 AM. Every alert annotation must include a <code>runbook</code> URL.</li>
                            <li><strong>Alerting on things that auto-recover:</strong> If a pod restarts and Kubernetes reschedules it within 30 seconds, paging a human adds no value. Alert only on conditions that persist beyond automatic recovery.</li>
                            <li><strong>Too many critical alerts (alert fatigue):</strong> When everything is critical, nothing is. Reserve critical/page severity for issues that directly impact users or revenue. Use warning/ticket severity for everything else.</li>
                            <li><strong>Alerting on causes instead of effects:</strong> "CPU is high" is a cause. "Request latency exceeds SLO" is the effect users feel. Alert on the effect; investigate the cause in the runbook.</li>
                            <li><strong>Copy-pasting thresholds:</strong> Every service has different baselines. A 500ms P95 might be normal for a report generator but catastrophic for an API gateway. Set thresholds based on actual SLOs, not arbitrary numbers.</li>
                        </ul>
                    </div>

                    <div class="alert alert-success">
                        <div class="alert-title">The ideal alert checklist</div>
                        <ul>
                            <li>Has a meaningful <code>for</code> duration (no flapping)</li>
                            <li>Includes <code>runbook</code> URL in annotations</li>
                            <li>Has clear <code>summary</code> and <code>description</code> with template variables</li>
                            <li>Severity matches impact (critical = user-facing, warning = degraded)</li>
                            <li>Requires immediate human action (not "wait and see")</li>
                            <li>Is reviewed and tuned at least quarterly</li>
                            <li>Has been tested with <code>amtool check-config</code> and dry-run routing</li>
                        </ul>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 07 — DASHBOARDS & VISUALIZATION
         ============================================ -->

    <section id="dashboards" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">07</span>
                <div class="section-header-text">
                    <h2 class="section-title">Dashboards &amp; Visualization</h2>
                    <p class="section-description">Dashboard design principles, essential dashboard types, Grafana panel types, template variables, and dashboard-as-code workflows.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Dashboard Design Principles -->
                <div class="subsection">
                    <h3 class="subsection-title">Dashboard Design Principles</h3>

                    <p>A dashboard is not a collection of random graphs. It is a visual argument about the health of a system. Every panel should answer a question, and every dashboard should tell a story. The goal is a <strong>5-second glance</strong> that tells you whether things are fine or need attention.</p>

                    <ul>
                        <li><strong>Z-pattern scanning:</strong> Users scan from top-left &rarr; top-right &rarr; bottom-left &rarr; bottom-right. Place the most critical panels (error rates, SLO status) in the top-left quadrant. Place detail panels and drill-downs in the lower sections.</li>
                        <li><strong>Visual hierarchy &mdash; larger panels = more important:</strong> A full-width time-series panel for request rate commands attention. A small stat panel for uptime is a supporting detail. Size encodes priority.</li>
                        <li><strong>5&ndash;7 panels per dashboard for quick comprehension:</strong> A dashboard with 30 panels requires scrolling and context-switching. Break large dashboards into linked sub-dashboards with drill-down links.</li>
                        <li><strong>Consistent color coding:</strong> Red means something is broken. Green means healthy. Yellow means warning or degraded. Never deviate from this convention &mdash; it leverages pre-attentive processing so viewers can assess status before conscious thought.</li>
                    </ul>

                    <h4 class="subsection-subtitle">Match Visualization Type to Data Type</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Data Type</th>
                                    <th>Best Visualization</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Trends over time</td>
                                    <td><strong>Time series</strong></td>
                                    <td>CPU usage, request rate, error count over 24h</td>
                                </tr>
                                <tr>
                                    <td>Current value</td>
                                    <td><strong>Stat / Gauge</strong></td>
                                    <td>Uptime %, current error rate, active connections</td>
                                </tr>
                                <tr>
                                    <td>Distribution</td>
                                    <td><strong>Heatmap</strong></td>
                                    <td>Latency percentiles, request size distribution</td>
                                </tr>
                                <tr>
                                    <td>Comparison</td>
                                    <td><strong>Bar chart</strong></td>
                                    <td>Service-to-service traffic, resource usage by pod</td>
                                </tr>
                                <tr>
                                    <td>Detail</td>
                                    <td><strong>Table</strong></td>
                                    <td>Active incidents, top endpoints by latency, error log entries</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- Essential Dashboard Types -->
                <div class="subsection">
                    <h3 class="subsection-title">Essential Dashboard Types</h3>

                    <p>Every team needs at least four dashboards covering different perspectives of system health. Each maps to a monitoring methodology and answers different questions.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Services</span>
                                RED Dashboard
                            </div>
                            <p class="command-card-desc">For each service: Request rate (throughput), Error rate as a percentage of total requests, and Duration as P50/P95/P99 latency. The RED method monitors the user experience of your service.</p>
                            <pre><code># Request Rate (per service)
sum(rate(http_requests_total{
  job="$service"
}[5m])) by (handler)

# Error Rate %
sum(rate(http_requests_total{
  job="$service",status=~"5.."}[5m]))
/
sum(rate(http_requests_total{
  job="$service"}[5m])) * 100

# P95 Latency
histogram_quantile(0.95,
  sum(rate(
    http_request_duration_seconds_bucket{
      job="$service"
    }[5m]
  )) by (le)
)</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Infrastructure</span>
                                USE Dashboard
                            </div>
                            <p class="command-card-desc">For infrastructure resources: Utilization (how busy is it), Saturation (how overloaded is it), and Errors (how often does it fail). Covers CPU, memory, disk, and network.</p>
                            <pre><code># CPU Utilization %
100 - (avg by (instance) (
  rate(node_cpu_seconds_total{
    mode="idle"
  }[5m])
) * 100)

# Memory Saturation (swap usage)
node_memory_SwapTotal_bytes
  - node_memory_SwapFree_bytes

# Disk Errors
rate(node_disk_io_time_weighted_seconds_total[5m])</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Reliability</span>
                                SLO Dashboard
                            </div>
                            <p class="command-card-desc">Error budget remaining over 30-day window, burn rate alerts, availability plotted over time. The SLO dashboard answers: &ldquo;Are we meeting our promises to users?&rdquo;</p>
                            <pre><code># Error Budget Remaining (%)
(1 - (
  sum(rate(http_requests_total{
    status=~"5.."}[30d]))
  /
  sum(rate(http_requests_total[30d]))
) / (1 - 0.999)) * 100

# Burn Rate (1h window)
sum(rate(http_requests_total{
  status=~"5.."}[1h]))
/
sum(rate(http_requests_total[1h]))
/
(1 - 0.999)</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Overview</span>
                                Golden Signals
                            </div>
                            <p class="command-card-desc">High-level system health at a glance. Combines latency, traffic, errors, and saturation into a single view. This is the dashboard you put on the team&rsquo;s wall TV.</p>
                            <pre><code># Latency (P95 across all services)
histogram_quantile(0.95,
  sum(rate(
    http_request_duration_seconds_bucket[5m]
  )) by (le))

# Traffic (total req/s)
sum(rate(http_requests_total[5m]))

# Error Rate (global)
sum(rate(http_requests_total{
  status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m])) * 100

# Saturation (avg CPU across fleet)
avg(100 - (avg by (instance) (
  rate(node_cpu_seconds_total{
    mode="idle"}[5m])) * 100))</code></pre>
                        </div>
                    </div>
                </div>

                <!-- Grafana Panel Types -->
                <div class="subsection">
                    <h3 class="subsection-title">Grafana Panel Types</h3>

                    <p>Grafana ships with a rich set of visualization panels. Choosing the right panel type for your data is critical &mdash; a heatmap for latency distribution conveys more insight than a time series of average latency.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Trend</span>
                                Time Series
                            </div>
                            <p class="command-card-desc">The workhorse panel. Shows values changing over time with lines, bars, or points. Supports multiple series, thresholds, and annotations. Use for CPU usage, request rate, error trends, and any metric that varies over time.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Value</span>
                                Stat
                            </div>
                            <p class="command-card-desc">Single big number with optional sparkline. Supports color thresholds (green/yellow/red) that change the background based on value ranges. Use for uptime percentage, current error rate, request count, or any single KPI.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Range</span>
                                Gauge
                            </div>
                            <p class="command-card-desc">Circular or bar gauge showing current value within a min/max range. Color thresholds indicate healthy, warning, and critical zones. Use for CPU utilization, memory usage, disk capacity, and any bounded metric.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Distribution</span>
                                Heatmap
                            </div>
                            <p class="command-card-desc">Two-dimensional visualization using color intensity to represent value density. Time on x-axis, bucket on y-axis, color = count. Use for latency distribution, request size patterns, and histogram data from Prometheus.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Detail</span>
                                Table
                            </div>
                            <p class="command-card-desc">Multi-column tabular display with sorting, filtering, and cell-level formatting. Supports links, thresholds, and unit formatting. Use for top-N endpoints, active incidents, pod status, or any multi-dimensional data.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Text</span>
                                Logs
                            </div>
                            <p class="command-card-desc">Integrated log viewing panel connected to Loki or Elasticsearch. Supports live tailing, field extraction, and log-to-trace links via trace IDs. Embeds log context directly alongside metric dashboards.</p>
                        </div>
                    </div>
                </div>

                <!-- Template Variables -->
                <div class="subsection">
                    <h3 class="subsection-title">Template Variables</h3>

                    <p>Template variables turn one dashboard into many. Instead of creating separate dashboards for each service, namespace, or environment, use dropdown variables that dynamically filter all panels. This eliminates dashboard sprawl and keeps your Grafana instance manageable.</p>

                    <p>Common variables every dashboard should include:</p>

                    <ul>
                        <li><strong><code>$namespace</code></strong> &mdash; Kubernetes namespace (production, staging, dev)</li>
                        <li><strong><code>$service</code></strong> &mdash; Service or job name, populated from label values</li>
                        <li><strong><code>$interval</code></strong> &mdash; PromQL range interval that adapts to the dashboard time range</li>
                        <li><strong><code>$instance</code></strong> &mdash; Specific instance for drill-down views</li>
                    </ul>

                    <h4 class="subsection-subtitle">Grafana Variable Definition (JSON)</h4>

<pre><code>{
  "templating": {
    "list": [
      {
        "name": "namespace",
        "type": "query",
        "datasource": "Prometheus",
        "query": "label_values(kube_pod_info, namespace)",
        "refresh": 2,
        "includeAll": true,
        "multi": true,
        "current": {
          "text": "production",
          "value": "production"
        }
      },
      {
        "name": "service",
        "type": "query",
        "datasource": "Prometheus",
        "query": "label_values(http_requests_total{namespace=\"$namespace\"}, job)",
        "refresh": 2,
        "includeAll": false,
        "multi": false
      },
      {
        "name": "interval",
        "type": "interval",
        "query": "1m,5m,15m,1h",
        "auto": true,
        "auto_min": "1m",
        "auto_count": 30
      }
    ]
  }
}</code></pre>

                    <div class="alert alert-info">
                        <div class="alert-title">Variable refresh settings</div>
                        <p>Set <code>"refresh": 2</code> (on time range change) for variables that depend on the selected time window. Use <code>"refresh": 1</code> (on dashboard load) for stable labels like namespace or cluster names. Avoid <code>"refresh": 0</code> (never) in production &mdash; stale variable values cause confusing empty panels.</p>
                    </div>
                </div>

                <!-- Dashboard as Code -->
                <div class="subsection">
                    <h3 class="subsection-title">Dashboard as Code</h3>

                    <p>Manually building dashboards in the Grafana UI is fine for prototyping, but production dashboards must be version-controlled and reproducible. Dashboard-as-code tools generate Grafana JSON from a higher-level language, enabling code review, templating, and CI/CD deployment.</p>

                    <h4 class="subsection-subtitle">Grafonnet (Jsonnet) Example</h4>

<pre><code>local grafonnet = import 'grafonnet-latest/main.libsonnet';
local dashboard = grafonnet.dashboard;
local panel = grafonnet.panel;
local prometheus = grafonnet.query.prometheus;

local requestRatePanel =
  panel.timeSeries.new('Request Rate')
  + panel.timeSeries.queryOptions.withTargets([
      prometheus.new(
        'Prometheus',
        'sum(rate(http_requests_total{job="$service"}[$interval])) by (handler)'
      )
      + prometheus.withLegendFormat('{{ handler }}'),
    ])
  + panel.timeSeries.standardOptions.withUnit('reqps')
  + panel.timeSeries.fieldConfig.defaults.custom.withFillOpacity(10);

local errorRatePanel =
  panel.stat.new('Error Rate')
  + panel.stat.queryOptions.withTargets([
      prometheus.new(
        'Prometheus',
        'sum(rate(http_requests_total{job="$service",status=~"5.."}[$interval]))
         / sum(rate(http_requests_total{job="$service"}[$interval])) * 100'
      ),
    ])
  + panel.stat.standardOptions.withUnit('percent')
  + panel.stat.options.withGraphMode('area');

grafonnet.dashboard.new('Service Overview')
+ dashboard.withUid('svc-overview')
+ dashboard.withTags(['generated', 'service'])
+ dashboard.withRefresh('30s')
+ dashboard.withPanels([
    requestRatePanel + panel.timeSeries.gridPos.withW(16) + panel.timeSeries.gridPos.withH(8),
    errorRatePanel + panel.stat.gridPos.withW(8) + panel.stat.gridPos.withH(8) + panel.stat.gridPos.withX(16),
  ])</code></pre>

                    <div class="alert alert-warning">
                        <div class="alert-title">Version-control your dashboards</div>
                        <p>Manual dashboard changes drift and get lost. Treat dashboards like infrastructure &mdash; define them in code, store them in Git, and deploy them through CI/CD. Use Grafana&rsquo;s provisioning feature or the API to push dashboard JSON from your repository. When a dashboard is modified in the UI, your CI pipeline should detect the drift and reconcile it.</p>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 08 — OPENTELEMETRY
         ============================================ -->

    <section id="opentelemetry" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">08</span>
                <div class="section-header-text">
                    <h2 class="section-title">OpenTelemetry</h2>
                    <p class="section-description">The vendor-neutral observability standard. Architecture, Collector configuration, auto-instrumentation, manual instrumentation, and essential environment variables.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Architecture Overview -->
                <div class="subsection">
                    <h3 class="subsection-title">Architecture Overview</h3>

                    <p>OpenTelemetry (OTel) is a CNCF incubating project that provides a single set of APIs, SDKs, and tools to instrument, generate, collect, and export telemetry data (metrics, logs, and traces). It is the merger of OpenCensus and OpenTracing and is now the second-most-active CNCF project after Kubernetes.</p>

                    <div class="trace-diagram">
<span class="span-name">Application</span>
<span class="span-id">├──</span> <span class="span-name">OTel SDK</span> <span class="span-id">(API + SDK)</span>
<span class="span-id">│   ├──</span> <span class="span-bar">Traces</span>   <span class="span-id">→</span> Span Processor    <span class="span-id">→</span> <span class="span-time">Exporter</span>
<span class="span-id">│   ├──</span> <span class="span-bar">Metrics</span>  <span class="span-id">→</span> Metric Reader     <span class="span-id">→</span> <span class="span-time">Exporter</span>
<span class="span-id">│   └──</span> <span class="span-bar">Logs</span>     <span class="span-id">→</span> Log Processor     <span class="span-id">→</span> <span class="span-time">Exporter</span>
<span class="span-id">└──</span> <span class="span-id">→</span> <span class="span-name">OTel Collector</span>
    <span class="span-id">├──</span> <span class="span-bar">Receivers</span>   <span class="span-id">(OTLP, Prometheus, Jaeger)</span>
    <span class="span-id">├──</span> <span class="span-bar">Processors</span>  <span class="span-id">(Batch, Filter, Attributes)</span>
    <span class="span-id">└──</span> <span class="span-bar">Exporters</span>   <span class="span-id">(OTLP, Prometheus, Jaeger, Loki)</span>
                    </div>

                    <p>The key architectural decision is whether to export telemetry directly from the SDK to a backend, or route it through the <strong>OTel Collector</strong>. In production, always use the Collector &mdash; it decouples your application from your backend choice, enables batching, retry, and filtering, and lets you switch backends without redeploying applications.</p>

                    <h4 class="subsection-subtitle">OTLP Protocol</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Transport</th>
                                    <th>Default Port</th>
                                    <th>Use Case</th>
                                    <th>Notes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>gRPC</strong></td>
                                    <td><code>4317</code></td>
                                    <td>Service-to-collector, high throughput</td>
                                    <td>Binary protobuf, bidirectional streaming, lower overhead</td>
                                </tr>
                                <tr>
                                    <td><strong>HTTP/protobuf</strong></td>
                                    <td><code>4318</code></td>
                                    <td>Browser, serverless, environments where gRPC is unavailable</td>
                                    <td>Protobuf over HTTP POST, easier to proxy and load-balance</td>
                                </tr>
                                <tr>
                                    <td><strong>HTTP/JSON</strong></td>
                                    <td><code>4318</code></td>
                                    <td>Debugging, manual testing with curl</td>
                                    <td>Human-readable but larger payload, slower serialization</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- Collector Configuration -->
                <div class="subsection">
                    <h3 class="subsection-title">Collector Configuration</h3>

                    <p>The OTel Collector is the central hub that receives, processes, and exports telemetry data. Its configuration is a single YAML file with four top-level sections: <code>receivers</code>, <code>processors</code>, <code>exporters</code>, and <code>service</code> (which wires them into pipelines).</p>

<pre><code># otel-collector-config.yaml
# ============================================
# Production-ready OTel Collector configuration
# ============================================

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 15s
          static_configs:
            - targets: ['localhost:8888']

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128
  resource:
    attributes:
      - key: environment
        value: production
        action: insert
      - key: collector.version
        value: "0.96.0"
        action: insert

exporters:
  otlp:
    endpoint: tempo:4317
    tls:
      insecure: true
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: otel
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
    labels:
      attributes:
        severity: ""
        service.name: ""

service:
  telemetry:
    logs:
      level: info
    metrics:
      address: 0.0.0.0:8888
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [otlp]
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus]
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [loki]</code></pre>

                    <div class="alert alert-info">
                        <div class="alert-title">Processor ordering matters</div>
                        <p>Processors execute in the order listed in the pipeline. Always put <code>memory_limiter</code> first &mdash; it protects the Collector from OOM kills by applying backpressure. Put <code>batch</code> after filtering processors so you batch only the data you intend to export.</p>
                    </div>
                </div>

                <!-- Auto-Instrumentation -->
                <div class="subsection">
                    <h3 class="subsection-title">Auto-Instrumentation</h3>

                    <p>Auto-instrumentation attaches agents or hooks at runtime to capture telemetry from popular frameworks and libraries without any code changes. It is the fastest way to get traces, metrics, and logs flowing from an existing application.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">JVM</span>
                                Java
                            </div>
                            <p class="command-card-desc">The Java agent attaches via <code>-javaagent</code> and instruments Spring Boot, gRPC, JDBC, Kafka, and 100+ libraries automatically.</p>
                            <pre><code># Download the agent
curl -L -o opentelemetry-javaagent.jar \
  https://github.com/open-telemetry/
    opentelemetry-java-instrumentation/
    releases/latest/download/
    opentelemetry-javaagent.jar

# Run with auto-instrumentation
java -javaagent:opentelemetry-javaagent.jar \
  -Dotel.service.name=order-service \
  -Dotel.exporter.otlp.endpoint=
    http://collector:4317 \
  -jar app.jar</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Interpreted</span>
                                Python
                            </div>
                            <p class="command-card-desc">The Python distro auto-detects Flask, Django, FastAPI, requests, psycopg2, and more. Zero code changes required.</p>
                            <pre><code># Install the distro + auto-instrumentors
pip install opentelemetry-distro
opentelemetry-bootstrap -a install

# Run with auto-instrumentation
OTEL_SERVICE_NAME=order-service \
OTEL_EXPORTER_OTLP_ENDPOINT=
  http://collector:4318 \
opentelemetry-instrument python app.py</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Runtime</span>
                                Node.js
                            </div>
                            <p class="command-card-desc">Requires a setup file loaded with <code>--require</code>. Auto-instruments Express, Fastify, pg, mysql2, Redis, and HTTP modules.</p>
                            <pre><code>// tracing.js — load before app
const { NodeSDK } = require(
  '@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations }
  = require('@opentelemetry/
    auto-instrumentations-node');
const { OTLPTraceExporter }
  = require('@opentelemetry/
    exporter-trace-otlp-http');

const sdk = new NodeSDK({
  traceExporter: new OTLPTraceExporter(),
  instrumentations: [
    getNodeAutoInstrumentations()
  ],
});
sdk.start();

// Run: node --require ./tracing.js app.js</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Compiled</span>
                                Go
                            </div>
                            <p class="command-card-desc">Go has no runtime agent mechanism. Instrumentation must be added manually using OTel SDK wrappers for net/http, gRPC, database/sql, etc.</p>
                            <pre><code>// Manual instrumentation required
// No auto-instrument agent for Go

// Use instrumentation libraries:
// go.opentelemetry.io/contrib/
//   instrumentation/net/http/otelhttp
// go.opentelemetry.io/contrib/
//   instrumentation/google.golang.org/
//   grpc/otelgrpc

import "go.opentelemetry.io/contrib/
  instrumentation/net/http/otelhttp"

handler := otelhttp.NewHandler(
  mux, "server")</code></pre>
                        </div>
                    </div>
                </div>

                <!-- Manual Instrumentation -->
                <div class="subsection">
                    <h3 class="subsection-title">Manual Instrumentation</h3>

                    <p>Auto-instrumentation captures framework-level telemetry, but business logic requires manual spans and custom metrics. Manual instrumentation adds the domain context that makes traces and metrics meaningful &mdash; order IDs, user types, payment amounts, feature flags.</p>

                    <h4 class="subsection-subtitle">Python &mdash; Traces with Custom Attributes and Metrics</h4>

<pre><code>from opentelemetry import trace, metrics

# Create a tracer and meter for this service
tracer = trace.get_tracer("order-service")
meter = metrics.get_meter("order-service")

# Define custom metrics
order_counter = meter.create_counter(
    "orders_processed",
    description="Total orders processed",
    unit="1"
)
order_duration = meter.create_histogram(
    "order_duration_ms",
    description="Time to process an order",
    unit="ms"
)

def process_order(order):
    with tracer.start_as_current_span("process-order") as span:
        # Add business context as span attributes
        span.set_attribute("order.id", order.id)
        span.set_attribute("order.total", order.total)
        span.set_attribute("order.item_count", len(order.items))
        span.set_attribute("customer.tier", order.customer.tier)

        try:
            result = execute_order(order)
            order_counter.add(1, {"status": "success", "tier": order.customer.tier})
            return result

        except Exception as e:
            # Record the exception on the span
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            order_counter.add(1, {"status": "error", "tier": order.customer.tier})
            raise</code></pre>

                    <h4 class="subsection-subtitle">Adding Child Spans for Sub-Operations</h4>

<pre><code>def execute_order(order):
    with tracer.start_as_current_span("validate-inventory") as span:
        span.set_attribute("warehouse.id", order.warehouse_id)
        check_inventory(order.items)

    with tracer.start_as_current_span("charge-payment") as span:
        span.set_attribute("payment.method", order.payment_method)
        span.set_attribute("payment.amount", order.total)
        charge_result = process_payment(order)
        span.set_attribute("payment.transaction_id", charge_result.txn_id)

    with tracer.start_as_current_span("send-confirmation") as span:
        span.set_attribute("notification.channel", "email")
        send_confirmation_email(order)

    return OrderResult(status="completed")</code></pre>
                </div>

                <!-- Environment Variables -->
                <div class="subsection">
                    <h3 class="subsection-title">Environment Variables</h3>

                    <p>OTel SDKs are configured primarily through environment variables, making it easy to change behavior without code changes. These variables work across all language SDKs, providing a consistent configuration interface.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Variable</th>
                                    <th>Purpose</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>OTEL_SERVICE_NAME</code></td>
                                    <td>Service identifier &mdash; the most important attribute. Appears in all telemetry and is used for service maps.</td>
                                    <td><code>"order-service"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_TRACES_EXPORTER</code></td>
                                    <td>Trace export format. <code>otlp</code> for Collector, <code>console</code> for debugging, <code>none</code> to disable.</td>
                                    <td><code>"otlp"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_METRICS_EXPORTER</code></td>
                                    <td>Metrics export format. Same options as traces.</td>
                                    <td><code>"otlp"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_EXPORTER_OTLP_ENDPOINT</code></td>
                                    <td>Collector URL. For gRPC use port 4317, for HTTP use 4318.</td>
                                    <td><code>"http://localhost:4318"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_TRACES_SAMPLER</code></td>
                                    <td>Sampling strategy. <code>always_on</code>, <code>always_off</code>, <code>traceidratio</code>, or <code>parentbased_traceidratio</code>.</td>
                                    <td><code>"parentbased_traceidratio"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_TRACES_SAMPLER_ARG</code></td>
                                    <td>Sample rate argument. <code>0.1</code> means 10% of traces are sampled.</td>
                                    <td><code>"0.1"</code></td>
                                </tr>
                                <tr>
                                    <td><code>OTEL_RESOURCE_ATTRIBUTES</code></td>
                                    <td>Comma-separated key=value pairs added to all telemetry as resource attributes.</td>
                                    <td><code>"env=prod,version=1.2.3"</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Kubernetes Deployment with OTel Environment Variables</h4>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  template:
    spec:
      containers:
        - name: order-service
          image: order-service:1.2.3
          env:
            - name: OTEL_SERVICE_NAME
              value: "order-service"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://otel-collector:4318"
            - name: OTEL_TRACES_SAMPLER
              value: "parentbased_traceidratio"
            - name: OTEL_TRACES_SAMPLER_ARG
              value: "0.1"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: "env=production,version=1.2.3,k8s.namespace=$(K8S_NAMESPACE)"
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace</code></pre>

                    <div class="alert alert-warning">
                        <div class="alert-title">Sampling in production</div>
                        <p>At scale, tracing 100% of requests is prohibitively expensive. Use <code>parentbased_traceidratio</code> with a rate of <code>0.1</code> (10%) as a starting point. The <code>parentbased</code> prefix ensures child spans inherit the parent&rsquo;s sampling decision, keeping traces complete. For critical paths (checkout, payment), use the Collector&rsquo;s tail-sampling processor to always capture errors and high-latency traces regardless of the head-sampling rate.</p>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 09 — INFRASTRUCTURE MONITORING
         ============================================ -->

    <section id="infrastructure" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">09</span>
                <div class="section-header-text">
                    <h2 class="section-title">Infrastructure Monitoring</h2>
                    <p class="section-description">Node Exporter for Linux systems, container monitoring with cAdvisor and kube-state-metrics, cloud provider integrations, and a complete Docker Compose monitoring stack.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Node Exporter -->
                <div class="subsection">
                    <h3 class="subsection-title">Node Exporter (Linux Systems)</h3>

                    <p>The Prometheus Node Exporter exposes hardware and OS-level metrics from Linux hosts. It is the standard way to collect CPU, memory, disk, network, and filesystem metrics from bare-metal servers, VMs, and Kubernetes nodes.</p>

                    <h4 class="subsection-subtitle">Installation &amp; Setup</h4>

<pre><code># Download and run Node Exporter
curl -LO https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz
tar xvfz node_exporter-*.tar.gz
cd node_exporter-*
./node_exporter --web.listen-address=":9100"

# Or run via Docker
docker run -d --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# Prometheus scrape config
# Add to prometheus.yml:
scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
    scrape_interval: 15s</code></pre>

                    <h4 class="subsection-subtitle">Key Metrics</h4>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Type</th>
                                    <th>Description</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>node_cpu_seconds_total</code></td>
                                    <td><span class="badge badge-teal">Counter</span></td>
                                    <td>CPU time spent in each mode (user, system, idle, iowait, etc.)</td>
                                </tr>
                                <tr>
                                    <td><code>node_memory_MemAvailable_bytes</code></td>
                                    <td><span class="badge badge-moss">Gauge</span></td>
                                    <td>Memory available for allocation without swapping</td>
                                </tr>
                                <tr>
                                    <td><code>node_filesystem_avail_bytes</code></td>
                                    <td><span class="badge badge-moss">Gauge</span></td>
                                    <td>Available disk space on each mounted filesystem</td>
                                </tr>
                                <tr>
                                    <td><code>node_network_receive_bytes_total</code></td>
                                    <td><span class="badge badge-teal">Counter</span></td>
                                    <td>Total bytes received on each network interface</td>
                                </tr>
                                <tr>
                                    <td><code>node_load1</code></td>
                                    <td><span class="badge badge-moss">Gauge</span></td>
                                    <td>1-minute load average &mdash; number of processes in runnable or uninterruptible state</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Essential PromQL Queries</h4>

<pre><code># CPU Usage % (across all cores)
100 - (avg by (instance) (
  rate(node_cpu_seconds_total{mode="idle"}[5m])
) * 100)

# Memory Usage %
(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

# Disk Usage %
(1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"}
  / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100

# Network Throughput (bytes/sec received)
rate(node_network_receive_bytes_total{device!="lo"}[5m])

# Network Throughput (bytes/sec transmitted)
rate(node_network_transmit_bytes_total{device!="lo"}[5m])

# Disk I/O Utilization %
rate(node_disk_io_time_seconds_total[5m]) * 100

# Predict disk full in 24 hours (linear regression)
predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"}[6h], 24*3600) &lt; 0</code></pre>
                </div>

                <!-- Container Monitoring -->
                <div class="subsection">
                    <h3 class="subsection-title">Container Monitoring</h3>

                    <p>Container environments add a layer of abstraction between your application and the host. cAdvisor (Container Advisor) collects resource usage and performance data from running containers. In Kubernetes, <strong>kube-state-metrics</strong> exposes cluster-level state as Prometheus metrics.</p>

                    <h4 class="subsection-subtitle">cAdvisor Metrics (Docker / containerd)</h4>

<pre><code># Container CPU Usage %
sum(rate(container_cpu_usage_seconds_total{
  name!="",container!="POD"
}[5m])) by (name) * 100

# Container Memory Usage
container_memory_working_set_bytes{
  name!="",container!="POD"
}

# Container Memory Usage % (vs limit)
container_memory_working_set_bytes{container!="POD"}
/
container_spec_memory_limit_bytes{container!="POD"} * 100

# Container Network I/O (received bytes/sec)
rate(container_network_receive_bytes_total[5m])

# Container Filesystem Usage
container_fs_usage_bytes{container!="POD"}
/
container_fs_limit_bytes{container!="POD"} * 100</code></pre>

                    <h4 class="subsection-subtitle">Kubernetes Metrics (kube-state-metrics)</h4>

<pre><code># Pods not in Running state
kube_pod_status_phase{phase!="Running",phase!="Succeeded"} == 1

# Container restarts (last hour)
increase(kube_pod_container_status_restarts_total[1h]) &gt; 3

# Deployment replicas not ready
kube_deployment_status_replicas_available
  != kube_deployment_spec_replicas

# Node conditions (DiskPressure, MemoryPressure, PIDPressure)
kube_node_status_condition{condition!="Ready",status="true"} == 1

# Resource requests vs limits vs actual usage
# (helps identify over/under-provisioned pods)
sum by (namespace, pod) (
  kube_pod_container_resource_requests{resource="memory"}
)

# Pending pods (scheduling failures)
kube_pod_status_phase{phase="Pending"} == 1</code></pre>
                </div>

                <!-- Cloud Provider Monitoring -->
                <div class="subsection">
                    <h3 class="subsection-title">Cloud Provider Monitoring</h3>

                    <p>Each major cloud provider has its own monitoring stack. The modern approach is to use OpenTelemetry as a unified collection layer and export to provider-native backends or your own Prometheus/Grafana stack.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">AWS</span>
                                CloudWatch
                            </div>
                            <p class="command-card-desc">Native AWS metrics for all services (EC2, RDS, Lambda, ECS, etc.). Custom metrics via PutMetricData API. CloudWatch Agent collects OS-level metrics and logs. Supports OTLP export via AWS Distro for OpenTelemetry (ADOT).</p>
                            <pre><code># Install ADOT Collector (ECS)
# Add as a sidecar container:
{
  "name": "otel-collector",
  "image": "public.ecr.aws/aws-
    observability/aws-otel-collector",
  "environment": [{
    "name": "AOT_CONFIG_CONTENT",
    "value": "... collector config ..."
  }]
}

# CloudWatch Metrics Insight query
SELECT AVG(CPUUtilization)
FROM SCHEMA("AWS/EC2", InstanceId)
WHERE InstanceId = 'i-0123456789'</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Azure</span>
                                Azure Monitor
                            </div>
                            <p class="command-card-desc">Application Insights for APM (traces, metrics, logs). Log Analytics workspace with KQL query language. Native OpenTelemetry SDK support through Azure Monitor Exporter. Integrates with Grafana via Azure Monitor data source.</p>
                            <pre><code>// Azure OpenTelemetry setup (C#)
using Azure.Monitor.OpenTelemetry
  .AspNetCore;

var builder =
  WebApplication.CreateBuilder(args);

builder.Services
  .AddOpenTelemetry()
  .UseAzureMonitor(options =&gt; {
    options.ConnectionString =
      "InstrumentationKey=...";
  });

// KQL query in Log Analytics
requests
| where resultCode &gt;= 500
| summarize count() by bin(
    timestamp, 5m), cloud_RoleName
| render timechart</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">GCP</span>
                                Cloud Monitoring
                            </div>
                            <p class="command-card-desc">Native OTLP support &mdash; GCP accepts OpenTelemetry data directly. Cloud Trace for distributed tracing. Cloud Logging with powerful filtering. Managed Prometheus (GMP) for PromQL-compatible metrics at scale.</p>
                            <pre><code># GCP Managed Prometheus
# Scrapes Prometheus metrics and stores
# in Cloud Monitoring backend.
# Use standard PromQL to query.

# Deploy GMP frontend
kubectl apply -f https://raw.githubusercontent
  .com/GoogleCloudPlatform/
  prometheus-engine/main/manifests/
  setup.yaml

# MQL (Monitoring Query Language)
fetch gce_instance
| metric 'compute.googleapis.com/
    instance/cpu/utilization'
| group_by [resource.instance_id]
| align rate(5m)
| every 1m</code></pre>
                        </div>
                    </div>
                </div>

                <!-- Docker Compose Stack -->
                <div class="subsection">
                    <h3 class="subsection-title">Docker Compose Monitoring Stack</h3>

                    <p>The following Docker Compose file sets up a complete monitoring stack for local development or small deployments. It includes all four pillars of the <strong>LGTM stack</strong>: Loki (logs), Grafana (visualization), Tempo (traces), and Mimir/Prometheus (metrics).</p>

<pre><code># docker-compose.monitoring.yaml
# ============================================
# Complete LGTM Monitoring Stack
# ============================================
version: '3.8'

services:
  # --- Metrics ---
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # --- Visualization ---
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
      - loki
      - tempo
    restart: unless-stopped

  # --- Logs ---
  loki:
    image: grafana/loki:latest
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  # --- Traces ---
  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    ports:
      - "3200:3200"     # Tempo API
      - "4317:4317"     # OTLP gRPC
      - "4318:4318"     # OTLP HTTP
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - tempo_data:/var/tempo
    restart: unless-stopped

  # --- Collector ---
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    ports:
      - "4317:4317"     # OTLP gRPC (if Tempo not using it)
      - "4318:4318"     # OTLP HTTP
      - "8889:8889"     # Prometheus exporter
      - "8888:8888"     # Collector metrics
    volumes:
      - ./otel-config.yaml:/etc/otel/config.yaml
    command: ["--config=/etc/otel/config.yaml"]
    depends_on:
      - prometheus
      - loki
      - tempo
    restart: unless-stopped

  # --- Host Metrics ---
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
  loki_data:
  tempo_data:</code></pre>

                    <div class="alert alert-success">
                        <div class="alert-title">The complete LGTM stack</div>
                        <p>This stack gives you metrics (Prometheus), logs (Loki), traces (Tempo), and dashboards (Grafana) &mdash; the complete LGTM stack. Point your applications at the OTel Collector on ports 4317 (gRPC) or 4318 (HTTP), and all three signal types flow through a single endpoint. Grafana connects to all backends and provides cross-signal correlation: click a metric spike to see correlated logs and traces.</p>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 10 — SLIs, SLOs & ERROR BUDGETS
         ============================================ -->

    <section id="slos" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">10</span>
                <div class="section-header-text">
                    <h2 class="section-title">SLIs, SLOs &amp; Error Budgets</h2>
                    <p class="section-description">Service Level Indicators, Objectives, and Agreements. Error budget math, burn rate alerting, and policies that balance reliability with development velocity.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Core Definitions -->
                <div class="subsection">
                    <h3 class="subsection-title">Core Definitions</h3>

                    <p><strong>SLI</strong> (Service Level Indicator) is a quantitative measure of some aspect of the level of service being provided. SLIs are the metrics that matter most to your users. Common examples include availability ratio, latency percentiles, throughput, and error rates.</p>

                    <p><strong>SLO</strong> (Service Level Objective) is a target value or range for an SLI. It defines the acceptable level of service. Examples: "99.9% of requests succeed over a 30-day rolling window" or "P95 latency &lt; 300ms."</p>

                    <p><strong>SLA</strong> (Service Level Agreement) is a business contract between a provider and a customer that specifies consequences (usually financial) when SLOs are not met. SLAs are legal commitments; SLOs are internal engineering targets.</p>

                    <div class="alert alert-warning">
                        <div class="alert-title">SLOs should be stricter than SLAs</div>
                        <p>The SLO is your <strong>internal</strong> target; the SLA is your <strong>external</strong> promise. If your SLA guarantees 99.9% availability, your SLO should target 99.95% or higher. This gives your team a buffer to detect and fix problems before they breach the SLA and trigger financial penalties.</p>
                    </div>

                    <p><strong>Error Budget</strong> is the allowed amount of unreliability. It is calculated as <code>100% - SLO%</code>. If your SLO is 99.9%, your error budget is 0.1% &mdash; meaning you can afford 0.1% of requests to fail (or 0.1% of time to be unavailable) within the measurement window.</p>
                </div>

                <!-- Error Budget Math -->
                <div class="subsection">
                    <h3 class="subsection-title">Error Budget Math</h3>

                    <p>The following table shows how SLO targets translate into concrete error budgets and allowed downtime. Small differences in SLO percentages have dramatic effects on permitted unavailability.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>SLO</th>
                                    <th>Error Budget</th>
                                    <th>Monthly Downtime</th>
                                    <th>Yearly Downtime</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>99%</strong></td>
                                    <td>1%</td>
                                    <td>7h 18m</td>
                                    <td>3d 15h</td>
                                </tr>
                                <tr>
                                    <td><strong>99.5%</strong></td>
                                    <td>0.5%</td>
                                    <td>3h 39m</td>
                                    <td>1d 19h</td>
                                </tr>
                                <tr>
                                    <td><strong>99.9%</strong></td>
                                    <td>0.1%</td>
                                    <td>43m 50s</td>
                                    <td>8h 46m</td>
                                </tr>
                                <tr>
                                    <td><strong>99.95%</strong></td>
                                    <td>0.05%</td>
                                    <td>21m 55s</td>
                                    <td>4h 23m</td>
                                </tr>
                                <tr>
                                    <td><strong>99.99%</strong></td>
                                    <td>0.01%</td>
                                    <td>4m 23s</td>
                                    <td>52m 36s</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Burn Rate</h4>

                    <p>Burn rate measures how fast you are consuming your error budget relative to the SLO window. A burn rate of 1 means you will exhaust your budget exactly at the end of the window. Higher burn rates indicate faster consumption and require more urgent response.</p>

<pre><code># Burn Rate Formula
# ============================================
Burn Rate = Actual Error Rate / Error Budget Rate

# Examples (assuming 30-day window, 99.9% SLO):
# Error Budget Rate = 0.1% = 0.001

Burn Rate 1    = sustainable (budget lasts full 30-day window)
Burn Rate 2    = budget exhausted in 15 days (half the window)
Burn Rate 10   = budget exhausted in 3 days
Burn Rate 14.4 = 2% of budget consumed in 1 hour
Burn Rate 36   = entire budget consumed in 20 hours</code></pre>
                </div>

                <!-- Defining SLIs -->
                <div class="subsection">
                    <h3 class="subsection-title">Defining SLIs</h3>

                    <p>SLIs should be measured as close to the user as possible. Use recording rules to pre-compute SLI ratios so that dashboards and alerts query pre-aggregated data rather than computing ratios on the fly.</p>

                    <h4 class="subsection-subtitle">Availability SLI</h4>

                    <p>The availability SLI measures the proportion of successful requests. A request is "successful" if it does not return a server error (5xx status code).</p>

<pre><code># Availability SLI — PromQL Recording Rule
# ============================================
# Ratio of non-5xx requests to total requests
# over a 5-minute window

- record: sli:availability:ratio_5m
  expr: |
    sum(rate(http_requests_total{status!~"5.."}[5m]))
    /
    sum(rate(http_requests_total[5m]))</code></pre>

                    <h4 class="subsection-subtitle">Latency SLI</h4>

                    <p>The latency SLI measures the proportion of requests that complete within an acceptable threshold. For example, "what percentage of requests finish in under 300ms?"</p>

<pre><code># Latency SLI — Percentage Under Threshold
# ============================================
# Ratio of requests completing in &lt; 300ms
# Uses histogram bucket boundaries

- record: sli:latency:ratio_5m
  expr: |
    sum(rate(http_request_duration_seconds_bucket{le="0.3"}[5m]))
    /
    sum(rate(http_request_duration_seconds_count[5m]))</code></pre>

                    <h4 class="subsection-subtitle">P95 / P99 Recording Rules</h4>

<pre><code># P95 Latency Recording Rule
- record: sli:latency:p95_5m
  expr: |
    histogram_quantile(0.95,
      sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
    )

# P99 Latency Recording Rule
- record: sli:latency:p99_5m
  expr: |
    histogram_quantile(0.99,
      sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
    )

# Per-service P95 (useful for SLO dashboards)
- record: sli:latency:p95_by_service_5m
  expr: |
    histogram_quantile(0.95,
      sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
    )</code></pre>
                </div>

                <!-- Multi-Window Burn Rate Alerts -->
                <div class="subsection">
                    <h3 class="subsection-title">Multi-Window Burn Rate Alerts</h3>

                    <p>Multi-window, multi-burn-rate alerting is the <strong>gold standard</strong> for SLO-based alerting. It uses two windows per severity level: a long window to detect sustained problems and a short window to confirm the problem is ongoing (reducing false positives from brief spikes). The short window is always 1/12 of the long window.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Severity</th>
                                    <th>Long Window</th>
                                    <th>Short Window</th>
                                    <th>Burn Rate</th>
                                    <th>Budget Consumed</th>
                                    <th>Action</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><span class="badge badge-clay">Critical</span></td>
                                    <td>1 hour</td>
                                    <td>5 min</td>
                                    <td>14.4x</td>
                                    <td>2% in 1h</td>
                                    <td>Page immediately</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-amber">Warning</span></td>
                                    <td>6 hours</td>
                                    <td>30 min</td>
                                    <td>6x</td>
                                    <td>5% in 6h</td>
                                    <td>Page on-call</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-teal">Info</span></td>
                                    <td>3 days</td>
                                    <td>6 hours</td>
                                    <td>1x</td>
                                    <td>10% in 3d</td>
                                    <td>Create ticket</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="subsection-subtitle">Recording Rules for Error Ratios</h4>

<pre><code># Recording rules for error ratios at multiple windows
# ============================================
# These pre-compute error ratios so alert rules
# can reference them cheaply

groups:
  - name: slo-error-ratios
    rules:
      # 5-minute window
      - record: slo:error_ratio:rate5m
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))

      # 30-minute window
      - record: slo:error_ratio:rate30m
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total[30m]))

      # 1-hour window
      - record: slo:error_ratio:rate1h
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))

      # 6-hour window
      - record: slo:error_ratio:rate6h
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total[6h]))

      # 3-day window
      - record: slo:error_ratio:rate3d
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[3d]))
          /
          sum(rate(http_requests_total[3d]))</code></pre>

                    <h4 class="subsection-subtitle">Complete Prometheus Alert Rules</h4>

<pre><code># Multi-window burn rate alert rules
# ============================================
# SLO: 99.9% availability (error budget = 0.001)

groups:
  - name: slo-burn-rate-alerts
    rules:
      # CRITICAL — 14.4x burn rate
      # 2% of 30-day budget consumed in 1 hour
      - alert: SLOBurnRateCritical
        expr: |
          slo:error_ratio:rate1h > (14.4 * 0.001)
          and
          slo:error_ratio:rate5m > (14.4 * 0.001)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High burn rate on availability SLO"
          description: "Error budget burn rate is 14.4x. 2% of monthly budget consumed in the last hour."

      # WARNING — 6x burn rate
      # 5% of 30-day budget consumed in 6 hours
      - alert: SLOBurnRateWarning
        expr: |
          slo:error_ratio:rate6h > (6 * 0.001)
          and
          slo:error_ratio:rate30m > (6 * 0.001)
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Elevated burn rate on availability SLO"
          description: "Error budget burn rate is 6x. 5% of monthly budget consumed in the last 6 hours."

      # INFO — 1x burn rate (ticket)
      # 10% of 30-day budget consumed in 3 days
      - alert: SLOBurnRateInfo
        expr: |
          slo:error_ratio:rate3d > (1 * 0.001)
          and
          slo:error_ratio:rate6h > (1 * 0.001)
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Slow burn on availability SLO"
          description: "Error budget burn rate is 1x. 10% of monthly budget consumed in the last 3 days."</code></pre>
                </div>

                <!-- Error Budget Policy -->
                <div class="subsection">
                    <h3 class="subsection-title">Error Budget Policy</h3>

                    <p>An error budget policy defines what actions the team takes based on how much error budget remains. It is the mechanism that translates SLO health into concrete engineering priorities.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Budget Remaining</th>
                                    <th>Status</th>
                                    <th>Action</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><span class="badge badge-moss">&gt; 20%</span></td>
                                    <td><strong>Healthy</strong></td>
                                    <td>Innovate freely. Ship features, experiment, take calculated risks with deployments.</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-amber">0 &ndash; 20%</span></td>
                                    <td><strong>Caution</strong></td>
                                    <td>Slow down releases. Increase testing coverage. Add canary deployments. Review recent changes for reliability risk.</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-clay">Exhausted</span></td>
                                    <td><strong>Frozen</strong></td>
                                    <td>Freeze all non-critical releases. Only P0 bug fixes and reliability improvements allowed. Focus entirely on restoring budget.</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-clay">Single incident &gt; 20%</span></td>
                                    <td><strong>Mandatory Review</strong></td>
                                    <td>Any single incident consuming more than 20% of the error budget triggers a mandatory postmortem regardless of remaining budget.</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="alert alert-info">
                        <div class="alert-title">Error budgets bridge velocity and reliability</div>
                        <p>Error budgets are the bridge between development velocity and reliability. When the budget is healthy, developers ship fast. When the budget is low, the team shifts focus to reliability. This creates a self-regulating system where both sides &mdash; feature development and operational stability &mdash; get the attention they need, driven by objective data rather than subjective arguments.</p>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 11 — INCIDENT RESPONSE
         ============================================ -->

    <section id="incidents" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">11</span>
                <div class="section-header-text">
                    <h2 class="section-title">Incident Response</h2>
                    <p class="section-description">Severity classification, incident lifecycle management, blameless postmortems, key reliability metrics, and communication best practices for when things go wrong.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- Severity Levels -->
                <div class="subsection">
                    <h3 class="subsection-title">Severity Levels</h3>

                    <p>A consistent severity classification ensures that incidents are escalated appropriately and that response expectations are clear across the organization.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Level</th>
                                    <th>Name</th>
                                    <th>Impact</th>
                                    <th>Response Time</th>
                                    <th>Notification</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><span class="badge badge-clay">SEV-1</span></td>
                                    <td><strong>Critical</strong></td>
                                    <td>Complete outage, data loss risk, or security breach</td>
                                    <td>&lt; 15 min</td>
                                    <td>Page entire team</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-amber">SEV-2</span></td>
                                    <td><strong>Major</strong></td>
                                    <td>Significant degradation affecting many users</td>
                                    <td>&lt; 30 min</td>
                                    <td>Page on-call</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-teal">SEV-3</span></td>
                                    <td><strong>Minor</strong></td>
                                    <td>Limited impact, workaround available</td>
                                    <td>&lt; 4 hours</td>
                                    <td>Slack / ticket</td>
                                </tr>
                                <tr>
                                    <td><span class="badge badge-moss">SEV-4</span></td>
                                    <td><strong>Low</strong></td>
                                    <td>Cosmetic issue, minor inconvenience</td>
                                    <td>Next business day</td>
                                    <td>Ticket only</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- Incident Lifecycle -->
                <div class="subsection">
                    <h3 class="subsection-title">Incident Lifecycle</h3>

                    <p>Every incident follows five phases. Clear ownership and defined activities at each phase prevent chaos and reduce resolution time.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Phase 1</span>
                                Detect
                            </div>
                            <p class="command-card-desc">Automated monitoring detects the anomaly. Alerts fire based on SLO burn rates, threshold breaches, or anomaly detection.</p>
                            <pre><code># Key activities:
- Alert fires (PagerDuty, OpsGenie)
- Automated diagnostics trigger
- On-call engineer is paged
- Incident channel is created

# Responsible: Monitoring system
# Goal: MTTD &lt; 5 minutes</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Phase 2</span>
                                Triage
                            </div>
                            <p class="command-card-desc">Assess severity, assign an Incident Commander (IC), and determine the scope of impact. Escalate if needed.</p>
                            <pre><code># Key activities:
- Assign severity (SEV-1 through SEV-4)
- Designate Incident Commander
- Identify affected systems/services
- Notify stakeholders
- Begin timeline documentation

# Responsible: On-call engineer / IC
# Goal: MTTA &lt; 15 minutes</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Phase 3</span>
                                Mitigate
                            </div>
                            <p class="command-card-desc">Stop the bleeding. Apply immediate fixes to restore service &mdash; rollback, failover, scale up, or apply hotfix. Root cause comes later.</p>
                            <pre><code># Key activities:
- Rollback recent deployments
- Failover to healthy replicas
- Scale up resources
- Apply config fixes / feature flags
- Communicate status updates

# Responsible: IC + responders
# Goal: Restore service ASAP</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Phase 4</span>
                                Resolve
                            </div>
                            <p class="command-card-desc">Confirm the root cause, deploy a permanent fix, and verify that all metrics have returned to normal baselines.</p>
                            <pre><code># Key activities:
- Identify root cause
- Deploy permanent fix
- Verify metrics are nominal
- Confirm with affected users
- Close incident channel

# Responsible: IC + engineering team
# Goal: Full resolution</code></pre>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Phase 5</span>
                                Learn
                            </div>
                            <p class="command-card-desc">Conduct a blameless postmortem. Document what happened, what went well, what failed, and define action items to prevent recurrence.</p>
                            <pre><code># Key activities:
- Schedule postmortem (within 48h)
- Write incident report
- Identify action items with owners
- Share learnings organization-wide
- Update runbooks and alerts

# Responsible: IC + all responders
# Goal: Prevent recurrence</code></pre>
                        </div>
                    </div>

                    <h4 class="subsection-subtitle">Incident Commander Responsibilities</h4>

                    <p>The Incident Commander (IC) is the single point of coordination during an incident. They do not fix the problem &mdash; they manage the response.</p>

<pre><code># Incident Commander Checklist
# ============================================

1. DECLARE the incident and assign severity
2. OPEN a dedicated incident channel (#inc-YYYY-MM-DD-title)
3. ASSEMBLE responders with relevant expertise
4. DELEGATE investigation to specific individuals
5. TRACK progress and maintain the timeline
6. COMMUNICATE status updates every 15-30 minutes
7. ESCALATE if resolution is not progressing
8. DECIDE when to declare mitigation / resolution
9. SCHEDULE the postmortem before closing the incident
10. HAND OFF if the incident outlasts your shift</code></pre>
                </div>

                <!-- Postmortem Process -->
                <div class="subsection">
                    <h3 class="subsection-title">Postmortem Process</h3>

                    <p>Blameless postmortems are the most important tool for building a culture of continuous improvement. They transform failures into organizational learning.</p>

                    <h4 class="subsection-subtitle">Blameless Culture Principles</h4>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">1</span>
                                Focus on Systems
                            </div>
                            <p class="command-card-desc">Analyze the systems, processes, and tools that allowed the failure to happen. Ask "what failed?" not "who failed?"</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">2</span>
                                Assume Good Intentions
                            </div>
                            <p class="command-card-desc">Everyone involved was doing their best with the information they had at the time. Hindsight bias is the enemy of learning.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">3</span>
                                Psychological Safety
                            </div>
                            <p class="command-card-desc">People must feel safe reporting errors and near-misses. If blame is the outcome, people will hide problems instead of surfacing them.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">4</span>
                                Continuous Improvement
                            </div>
                            <p class="command-card-desc">Every postmortem must produce concrete, assigned, and time-bound action items. Track completion and follow up.</p>
                        </div>
                    </div>

                    <h4 class="subsection-subtitle">Postmortem Template</h4>

<pre><code># Incident Postmortem: [Title]
# ============================================

Date: YYYY-MM-DD
Severity: SEV-X
Duration: X hours Y minutes
Impact: N users affected, M% request degradation

## Timeline
- HH:MM - Alert triggered (source: Prometheus/PagerDuty)
- HH:MM - Incident Commander assigned (@name)
- HH:MM - Root cause identified
- HH:MM - Mitigation deployed (rollback/hotfix/scaling)
- HH:MM - Service fully restored
- HH:MM - Incident resolved and channel closed

## Root Cause
[Systems-focused analysis of what broke and why.
 Include contributing factors and the chain of events.]

## What Went Well
- Detection was fast (&lt; 3 min MTTD)
- Runbook was accurate and up-to-date
- Communication was clear and timely

## What Didn't Go Well
- Rollback took too long (no automated rollback)
- Alert was too noisy, initially ignored
- Staging did not catch this failure mode

## Action Items
- [ ] Implement automated rollback (Owner: @name, Due: DATE)
- [ ] Update runbook for service X (Owner: @name, Due: DATE)
- [ ] Add integration test for failure mode (Owner: @name, Due: DATE)
- [ ] Tune alert threshold to reduce noise (Owner: @name, Due: DATE)</code></pre>
                </div>

                <!-- Key Metrics -->
                <div class="subsection">
                    <h3 class="subsection-title">Key Metrics &mdash; MTTD, MTTA, MTTR, MTBF</h3>

                    <p>These four metrics quantify your incident response capability. Track them over time to measure improvement and set targets for each severity level.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Full Name</th>
                                    <th>Formula</th>
                                    <th>Target</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>MTTD</strong></td>
                                    <td>Mean Time to Detect</td>
                                    <td><code>Alert time - Incident start time</code></td>
                                    <td>&lt; 5 min</td>
                                </tr>
                                <tr>
                                    <td><strong>MTTA</strong></td>
                                    <td>Mean Time to Acknowledge</td>
                                    <td><code>Ack time - Alert time</code></td>
                                    <td>&lt; 15 min</td>
                                </tr>
                                <tr>
                                    <td><strong>MTTR</strong></td>
                                    <td>Mean Time to Resolve</td>
                                    <td><code>Resolution time - Detection time</code></td>
                                    <td>&lt; 1 hour (SEV-1)</td>
                                </tr>
                                <tr>
                                    <td><strong>MTBF</strong></td>
                                    <td>Mean Time Between Failures</td>
                                    <td><code>Total uptime / Number of failures</code></td>
                                    <td>Maximize</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

<pre><code># Calculating incident response metrics
# ============================================

# MTTD — How fast do we detect problems?
MTTD = avg(alert_fired_timestamp - incident_start_timestamp)

# MTTA — How fast do we acknowledge alerts?
MTTA = avg(ack_timestamp - alert_fired_timestamp)

# MTTR — How fast do we resolve incidents?
MTTR = avg(resolved_timestamp - detected_timestamp)

# MTBF — How often do we have failures?
MTBF = total_uptime_hours / number_of_incidents

# Availability from MTTR and MTBF:
Availability = MTBF / (MTBF + MTTR)</code></pre>
                </div>

                <!-- Communication During Incidents -->
                <div class="subsection">
                    <h3 class="subsection-title">Communication During Incidents</h3>

                    <p>Effective communication during incidents reduces confusion, prevents duplicate effort, and keeps stakeholders informed. Establish clear communication channels and cadences before incidents happen.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Channel</span>
                                Dedicated Incident Channel
                            </div>
                            <p class="command-card-desc">Create a dedicated Slack/Teams channel for each SEV-1 or SEV-2 incident. Name it consistently: <code>#inc-YYYY-MM-DD-brief-title</code>. Pin the incident summary at the top.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Cadence</span>
                                Regular Status Updates
                            </div>
                            <p class="command-card-desc">SEV-1: Update every 15 minutes. SEV-2: Update every 30 minutes. Include current status, what is being tried, and expected next update time. Silence breeds anxiety.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Audience</span>
                                Internal vs. External
                            </div>
                            <p class="command-card-desc">Internal: Full technical details in the incident channel. External: Simple, empathetic language on the status page. Never blame specific components publicly.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Status Page</span>
                                Public Updates
                            </div>
                            <p class="command-card-desc">Update the status page within 5 minutes of confirming impact. Use three states: Investigating, Identified, Resolved. Tools: Statuspage.io, Cachet, Instatus.</p>
                        </div>
                    </div>

<pre><code># Status update template (internal)
# ============================================
# Post this in the incident channel at each update interval

**Incident Update — [HH:MM UTC]**
**Status:** Investigating / Mitigating / Monitoring
**Impact:** [Who/what is affected, % of users]
**Current actions:** [What we're doing right now]
**Next update:** [HH:MM UTC]
**IC:** @name

# Status update template (external / status page)
# ============================================
**[HH:MM UTC] — Investigating**
We are aware of issues affecting [service].
Our team is actively investigating.
We will provide an update within [30 minutes].

**[HH:MM UTC] — Identified**
The issue has been identified and a fix is being deployed.
Some users may experience [specific impact].

**[HH:MM UTC] — Resolved**
The issue has been resolved. All services are
operating normally. We will publish a postmortem
within 48 hours.</code></pre>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         SECTION 12 — TOOL ECOSYSTEM
         ============================================ -->

    <section id="ecosystem" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-number">12</span>
                <div class="section-header-text">
                    <h2 class="section-title">Tool Ecosystem</h2>
                    <p class="section-description">The Grafana LGTM stack, commercial platforms, cloud-native tools, comparison matrices, decision frameworks, and emerging trends in observability tooling.</p>
                </div>
            </div>

            <div class="section-content">

                <!-- The Grafana Stack (LGTM) -->
                <div class="subsection">
                    <h3 class="subsection-title">The Grafana Stack (LGTM)</h3>

                    <p>The Grafana LGTM stack is the most widely adopted open-source observability platform. Each component handles one signal type, and Grafana unifies them into a single pane of glass with cross-signal correlation.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Metrics</span>
                                Prometheus / Mimir
                            </div>
                            <p class="command-card-desc">Pull-based metrics collection with the PromQL query language. Mimir provides horizontally scalable long-term storage for Prometheus metrics.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Visualization</span>
                                Grafana
                            </div>
                            <p class="command-card-desc">Dashboarding and visualization for all signal types. 100+ data source plugins. Alerting, annotations, and team-based access control built in.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Logs</span>
                                Loki
                            </div>
                            <p class="command-card-desc">Log aggregation inspired by Prometheus. Indexes labels (not content), making it 10x cheaper than Elasticsearch for most workloads. LogQL query language.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Traces</span>
                                Tempo
                            </div>
                            <p class="command-card-desc">Distributed tracing backend with minimal indexing. Accepts Jaeger, Zipkin, and OTLP formats. Object storage backend for cost efficiency.</p>
                        </div>
                    </div>

                    <div class="alert alert-info">
                        <div class="alert-title">Best for</div>
                        <p>Cost-sensitive organizations, Kubernetes-native environments, and teams with SRE expertise. The LGTM stack offers maximum customization and zero licensing cost, but requires significant operational investment to deploy, tune, and maintain at scale.</p>
                    </div>
                </div>

                <!-- Commercial Platforms -->
                <div class="subsection">
                    <h3 class="subsection-title">Commercial Platforms</h3>

                    <p>Commercial observability platforms trade cost for convenience. They handle infrastructure, scaling, and maintenance, letting your team focus on instrumentation and analysis rather than platform operations.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">SaaS</span>
                                Datadog
                            </div>
                            <p class="command-card-desc">All-in-one SaaS observability platform with 750+ integrations. Excellent developer experience with auto-discovery, APM, log management, and infrastructure monitoring. Best-in-class UX but expensive at scale &mdash; costs grow linearly with hosts and log volume.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">SaaS</span>
                                New Relic
                            </div>
                            <p class="command-card-desc">Full-stack observability with a generous free tier (100 GB/month). Developer-friendly with strong APM, browser monitoring, and AI-powered anomaly detection. Popular in the mid-market for its balance of features and pricing.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">Enterprise</span>
                                Splunk
                            </div>
                            <p class="command-card-desc">Industry leader in log analytics with powerful search processing language (SPL). ML-driven anomaly detection, enterprise-grade security features, and compliance certifications. Strong in regulated industries and large enterprises.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">SaaS</span>
                                Honeycomb
                            </div>
                            <p class="command-card-desc">OpenTelemetry-native observability focused on high-cardinality tracing and debugging. BubbleUp feature automatically surfaces anomalous dimensions. Built for teams that prioritize deep debugging over broad dashboarding.</p>
                        </div>
                    </div>
                </div>

                <!-- Cloud-Native Tools -->
                <div class="subsection">
                    <h3 class="subsection-title">Cloud-Native Tools</h3>

                    <p>Every major cloud provider offers native monitoring tools. These integrate deeply with the provider's services but create vendor lock-in and typically offer limited multi-cloud support.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">AWS</span>
                                CloudWatch + X-Ray
                            </div>
                            <p class="command-card-desc">Native AWS metrics, logs, and alarms with zero setup for AWS services. X-Ray provides distributed tracing but is being sunset in favor of OpenTelemetry-based solutions. Migrate to OTel SDK with CloudWatch OTLP endpoint.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Azure</span>
                                Azure Monitor
                            </div>
                            <p class="command-card-desc">Application Insights for APM, Log Analytics workspace for centralized logging, and Azure Monitor Metrics for infrastructure. Native integration with all Azure services and strong Kusto Query Language (KQL) for log analysis.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">GCP</span>
                                Cloud Monitoring
                            </div>
                            <p class="command-card-desc">Native OTLP ingestion endpoint, Managed Service for Prometheus (drop-in Prometheus replacement with no infrastructure management), and Cloud Trace for distributed tracing. Leading cloud provider for OpenTelemetry support.</p>
                        </div>
                    </div>
                </div>

                <!-- Comparison Matrix -->
                <div class="subsection">
                    <h3 class="subsection-title">Comparison Matrix</h3>

                    <p>Use this matrix to compare observability platforms across the dimensions that matter most for your team and organization.</p>

                    <div class="table-wrapper">
                        <table>
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>Grafana Stack</th>
                                    <th>Datadog</th>
                                    <th>CloudWatch</th>
                                    <th>Elastic</th>
                                    <th>Honeycomb</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Type</strong></td>
                                    <td>Open Source</td>
                                    <td>SaaS</td>
                                    <td>Cloud-Native</td>
                                    <td>Open / Commercial</td>
                                    <td>SaaS</td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td><span class="badge badge-moss">Free + ops</span></td>
                                    <td><span class="badge badge-clay">$$$$$</span></td>
                                    <td><span class="badge badge-amber">$$$</span></td>
                                    <td><span class="badge badge-amber">$$$</span></td>
                                    <td><span class="badge badge-amber">$$$</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Setup Time</strong></td>
                                    <td>Weeks</td>
                                    <td>Days</td>
                                    <td>Days</td>
                                    <td>Weeks</td>
                                    <td>Days</td>
                                </tr>
                                <tr>
                                    <td><strong>Best Signal</strong></td>
                                    <td>Metrics</td>
                                    <td>All</td>
                                    <td>Logs</td>
                                    <td>Logs</td>
                                    <td>Traces</td>
                                </tr>
                                <tr>
                                    <td><strong>Multi-cloud</strong></td>
                                    <td><span class="badge badge-moss">Excellent</span></td>
                                    <td><span class="badge badge-moss">Excellent</span></td>
                                    <td><span class="badge badge-clay">AWS only</span></td>
                                    <td><span class="badge badge-moss">Excellent</span></td>
                                    <td><span class="badge badge-moss">Excellent</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Customization</strong></td>
                                    <td>Maximum</td>
                                    <td>Medium</td>
                                    <td>Low</td>
                                    <td>High</td>
                                    <td>Low</td>
                                </tr>
                                <tr>
                                    <td><strong>Learning Curve</strong></td>
                                    <td><span class="badge badge-clay">Steep</span></td>
                                    <td><span class="badge badge-moss">Low</span></td>
                                    <td><span class="badge badge-amber">Medium</span></td>
                                    <td><span class="badge badge-clay">Steep</span></td>
                                    <td><span class="badge badge-amber">Medium</span></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- Decision Framework -->
                <div class="subsection">
                    <h3 class="subsection-title">Decision Framework</h3>

                    <p>There is no single "best" observability platform. The right choice depends on your team's expertise, budget, infrastructure, and operational maturity.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Open Source</span>
                                Choose If...
                            </div>
                            <p class="command-card-desc">Your budget is limited but you have strong SRE expertise. You need maximum control over data retention, sampling, and pipeline configuration. You run multi-cloud or hybrid infrastructure and cannot accept vendor lock-in. You need to comply with data residency requirements.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Commercial</span>
                                Choose If...
                            </div>
                            <p class="command-card-desc">You need rapid implementation (days, not weeks). Your team has limited observability expertise and needs guided setup. You want enterprise support with SLAs. You value integrated AI/ML anomaly detection and need a single vendor for all signals.</p>
                        </div>
                    </div>

                    <div class="alert alert-success">
                        <div class="alert-title">Start with OpenTelemetry for instrumentation</div>
                        <p>Regardless of which backend you choose, instrument your applications using <strong>OpenTelemetry</strong>. It is vendor-neutral &mdash; you can switch backends later without re-instrumenting your code. OTel is the universal standard supported by every major observability vendor, and it decouples your instrumentation investment from your platform choice.</p>
                    </div>
                </div>

                <!-- 2026 Trends -->
                <div class="subsection">
                    <h3 class="subsection-title">2026 Trends</h3>

                    <p>The observability landscape is evolving rapidly. These are the key trends shaping the industry and the tools teams are adopting.</p>

                    <div class="command-grid">
                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">Standard</span>
                                OpenTelemetry as Universal Standard
                            </div>
                            <p class="command-card-desc">OTel has become the de facto instrumentation standard. All major vendors now accept OTLP natively. Proprietary agents are being phased out in favor of OTel SDKs and the Collector.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-amber">Migration</span>
                                AWS X-Ray Sunset
                            </div>
                            <p class="command-card-desc">AWS is migrating from X-Ray to OpenTelemetry-based tracing. Teams using X-Ray SDKs should migrate to OTel SDKs with the CloudWatch OTLP endpoint for future-proof instrumentation.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-moss">Kernel</span>
                                eBPF for Observability
                            </div>
                            <p class="command-card-desc">eBPF enables kernel-level observability without modifying application code. Tools like Cilium, Pixie, and Grafana Beyla provide zero-instrumentation tracing, network monitoring, and security observability.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-navy">AI/ML</span>
                                AI-Driven Anomaly Detection
                            </div>
                            <p class="command-card-desc">Machine learning models are increasingly used for automated anomaly detection, root cause analysis, and alert correlation. Platforms like Datadog, New Relic, and Dynatrace embed AI assistants that can explain incidents and suggest fixes.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-teal">GitOps</span>
                                Observability-as-Code
                            </div>
                            <p class="command-card-desc">Dashboards, alerts, SLOs, and recording rules are managed as code in Git repositories. Tools like Grafana Terraform provider, Crossplane, and jsonnet enable version-controlled, reviewable observability configuration.</p>
                        </div>

                        <div class="command-card">
                            <div class="command-card-title">
                                <span class="badge badge-clay">Cost</span>
                                Cost Optimization
                            </div>
                            <p class="command-card-desc">As telemetry volumes grow, cost optimization is becoming critical. Teams are adopting adaptive sampling, metric aggregation, log filtering at the Collector level, and tiered storage to control observability spend without losing visibility.</p>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- ============================================
         FOOTER
         ============================================ -->

    <footer class="site-footer">
        <p>Monitoring &amp; Observability &mdash; Atlas Console Design System</p>
        <a href="index.html">&larr; Back to Tech Guides</a>
    </footer>

</body>
</html>
